{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "13-introducing-automatic-optimization.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOMGEaOA78b2BvfvwviipXU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/grokking-deep-learning/blob/13-introducing-automatic-optimization/13_introducing_automatic_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKQqFDW4zOyt",
        "colab_type": "text"
      },
      "source": [
        "# Introducing automatic optimization: let’s build a deep learning framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLTada_-zSvz",
        "colab_type": "text"
      },
      "source": [
        "It’s extremely important for you to know what’s going on under the hood of these frameworks by implementing algorithms yourself (from scratch in NumPy). But now we’re going to transition into using a framework, because the networks you’ll be training next—long shortterm memory networks (LSTMs)—are very complex, and NumPy code describing their implementation is difficult to read, use, or debug (gradients are flying everywhere).\n",
        "\n",
        "It’s exactly this code complexity that deep learning frameworks were created to mitigate. **Especially if you wish to train a neural network on a GPU (giving 10–100× faster training), a deep learning framework can significantly reduce code complexity (reducing errors and increasing development speed) while also increasing runtime performance.**\n",
        "\n",
        "For these reasons, their use is nearly universal within the research community, and a thorough understanding of a deep learning framework will be essential on your journey toward becoming a user or researcher of deep learning.\n",
        "\n",
        "This way, you’ll have no doubt about what frameworks do when\n",
        "using them for complex architectures. Furthermore, building a small framework yourself should provide a smooth transition to using actual deep learning frameworks, because you’ll already be familiar with the API and the functionality underneath it.\n",
        "\n",
        "**Abstractly, it eliminates the need to write code that you’d repeat multiple times. Concretely, the most beneficial pieces of a deep learning\n",
        "framework are its support for automatic backpropagation and automatic optimization. These features let you specify only the forward propagation code of a model, with the framework taking care of backpropagation and weight updates automatically. Most frameworks even make the forward propagation code easier by providing high-level interfaces to common layers and loss functions.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pESZcC270o7u",
        "colab_type": "text"
      },
      "source": [
        "## Introduction to tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19_UwhuJ0rVY",
        "colab_type": "text"
      },
      "source": [
        "**Tensors are an abstract form of vectors and matrices.**\n",
        "\n",
        "Up to this point, we’ve been working exclusively with vectors and matrices as the basic data structures for deep learning. Recall that **a matrix is a list of vectors, and a vector is a list of scalars (single numbers). A tensor is the abstract version of this form of nested lists of numbers. A vector is a one-dimensional tensor. A matrix is a two-dimensional tensor, and higher dimensions are referred to as n-dimensional tensors**. Thus, the beginning of a new deep learning framework is the construction of this basic type, which we’ll call Tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk3Jf_761adO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oitoULz51h29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tensor(object):\n",
        "\n",
        "  def __init__(self, data):\n",
        "    self.data = np.array(data)\n",
        "\n",
        "  def __add__(self, other):\n",
        "    return Tensor(self.data + other.data)\n",
        "\n",
        "  def __repr__(self):\n",
        "    return str(self.data.__repr__())\n",
        "\n",
        "  def __str__(self):\n",
        "    return str(self.data.__str__())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0KAP7MN2qhA",
        "colab_type": "code",
        "outputId": "167ff0a5-c3b9-4b9b-c382-555e53f70e44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = Tensor([1, 2, 3, 4, 5])\n",
        "print(x)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 3 4 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzkwYvuj2xOv",
        "colab_type": "code",
        "outputId": "3958d471-5fcd-47cc-9c73-2e0c1fe7436c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y = x + x\n",
        "print(y)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2  4  6  8 10]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFDZRYa_3Dgw",
        "colab_type": "text"
      },
      "source": [
        "Note that it stores all the numerical information in a NumPy array (self.data), and it supports one tensor operation (addition). Adding more operations is relatively simple: create more functions on the tensor class with the appropriate functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlM152Vy3IzB",
        "colab_type": "text"
      },
      "source": [
        "## Introduction to automatic gradient computation(autograd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz7bMd2G3dUq",
        "colab_type": "text"
      },
      "source": [
        "**Stop! you performed backpropagation by hand.\n",
        "Let’s make it automatic!**\n",
        "\n",
        "You learned about derivatives. Since then, you’ve been computing derivatives\n",
        "by hand for each neural network you train. Recall that this is done by moving backward through the neural network: **first compute the gradient at the output of the network, then use that result to compute the derivative at the next-to-last component, and so on until all weights in the architecture have correct gradients. This logic for computing gradients can also be added to the tensor object.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiTFVteg22WC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tensor(object):\n",
        "\n",
        "  def __init__(self, data, creators=None, creation_op=None):\n",
        "    self.data = np.array(data)\n",
        "    self.creation_op = creation_op\n",
        "    self.creators = creators\n",
        "    self.grad = None\n",
        "\n",
        "  def backward(self, grad):\n",
        "    self.grad = grad\n",
        "\n",
        "    if (self.creation_op == 'add'):\n",
        "      self.creators[0].backward(grad)\n",
        "      self.creators[1].backward(grad)\n",
        "\n",
        "  def __add__(self, other):\n",
        "    return Tensor(self.data + other.data, creators=[self, other], creation_op='add')\n",
        "\n",
        "  def __repr__(self):\n",
        "    return str(self.data.__repr__())\n",
        "\n",
        "  def __str__(self):\n",
        "    return str(self.data.__str__())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQXKSrCb5EgH",
        "colab_type": "code",
        "outputId": "486d7135-18da-470f-8a5d-e99c88d5b15b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = Tensor([1, 2, 3, 4, 5])\n",
        "y = Tensor([2, 2, 2, 2, 2])\n",
        "print(x, y)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 3 4 5] [2 2 2 2 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LORe848Y5Wl6",
        "colab_type": "code",
        "outputId": "5b88255a-71f0-4173-8812-ce03342844bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "z = x + y\n",
        "print(z)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3 4 5 6 7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzsn5Yv85hA4",
        "colab_type": "code",
        "outputId": "9dd831cf-8719-4d7b-fc1e-f63f30ad20ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(z.backward(Tensor(np.array([1, 1, 1, 1, 1]))))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sU9HY6D_5yzI",
        "colab_type": "text"
      },
      "source": [
        "This method introduces two new concepts. First, each tensor gets two new attributes. creators is a list containing any tensors used in the creation of the current tensor (which defaults to None). Thus, when the two tensors $x$ and $y$ are added together, $z$ has two creators, $x$ and $y$. creation_op is a related feature that stores the instructions creators used in the creation process. \n",
        "\n",
        "**Thus, performing $z = x + y$ creates a computation graph with\n",
        "three nodes ($x$, $y$, and $z$) and two edges ($z -> x$ and $z -> y$). Each edge is labeled by the creation_op add. This graph allows you to recursively backpropagate gradients.**\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/grokking-deep-learning/node-graph.png?raw=1' width='800'/>\n",
        "\n",
        "**The first new concept in this implementation is the automatic creation of this graph whenever you perform math operations. If you took $z$ and performed further operations, the graph would continue with whatever resulting new variables pointed back to $z$.**\n",
        "\n",
        "**The second new concept introduced in this version of Tensor is the ability to use this graph to compute gradients.** When you call $z.backward()$, it sends the correct gradient for $x$ and $y$ given the function that was applied to create $z(add)$.\n",
        "\n",
        "Looking at the graph, you place a vector of gradients (np.array([1,1,1,1,1])) on $z$, and then they’re applied to their parents. As you know, backpropagating through addition means also applying addition when backpropagating. \n",
        "\n",
        "In this case, because there’s only one gradient to add into $x$\n",
        "or $y$, you copy the gradient from $z$ onto $x$ and $y$:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2UNGYKN8Yzm",
        "colab_type": "code",
        "outputId": "ed8a3218-fb5e-4874-bc5c-b44edeb765da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(x.grad)\n",
        "print(y.grad)\n",
        "print(z.creators)\n",
        "print(z.creation_op)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 1 1]\n",
            "[1 1 1 1 1]\n",
            "[array([1, 2, 3, 4, 5]), array([2, 2, 2, 2, 2])]\n",
            "add\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rptYWXUI87M1",
        "colab_type": "text"
      },
      "source": [
        "Perhaps the most elegant part of this form of autograd is that it works recursively as well, because each vector calls .backward() on all of its self.creators:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QffF0Qjh8wH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = Tensor([1, 2, 3, 4, 5])\n",
        "b = Tensor([2, 2, 2, 2, 2])\n",
        "c = Tensor([5, 4, 3, 2, 1])\n",
        "d = Tensor([-1, -2, -3, -4, -5])\n",
        "\n",
        "e = a + b\n",
        "f = c + d\n",
        "g = e + f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbLPtaW09mGO",
        "colab_type": "code",
        "outputId": "fe7c7f7b-06d7-40b4-9b10-0835c5d95fb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "g.backward(Tensor(np.array([1, 1, 1, 1, 1])))\n",
        "print(a.grad)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmummaq9xqG",
        "colab_type": "text"
      },
      "source": [
        "## A quick checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCwqcHgS9yhx",
        "colab_type": "text"
      },
      "source": [
        "**Everything in Tensor is another form of lessons already learned.**\n",
        "\n",
        "Before moving on, I want to first acknowledge that even if it feels like a bit of a stretch or a heavy lift to think about gradients flowing over a graphical structure, this is nothing new compared to what you’ve already been working with. In the previous chapter on RNNs, you forward propagated in one direction and then back propagated across a (virtual graph) of activations.\n",
        "\n",
        "In particular, this notion of a graph that gets built during forward propagation is called a dynamic computation graph because it’s built\n",
        "on the fly during forward prop. This is the type of autograd present in newer deep learning frameworks such as DyNet and PyTorch. Older frameworks such as Theano and TensorFlow have what’s called a static computation graph, which is specified before forward propagation even begins.\n",
        "\n",
        "In general, dynamic computation graphs are easier to write/experiment with, and static computation graphs have faster runtimes because of some fancy logic under the hood. But note that dynamic and static frameworks have lately been moving toward the middle, allowing dynamic graphs to compile to static ones (for faster runtimes) or allowing static graphs to be built dynamically (for easier experimentation). In the long run, you’re likely to end up with both. The primary difference is whether forward propagation is happening during graph construction or after the graph is already defined.\n",
        "\n",
        "The main point, here, is to help prepare you for deep learning in the real world, where 10% (or less) of your time will be spent thinking up a new idea and 90% of your time will be spent figuring out how to get a deep learning framework to play nicely. Debugging these frameworks can be extremely difficult at times, because most bugs don’t raise an error and print out a stack trace. Most bugs lie hidden within the code, keeping the network from training as it should (even if it appears to be training somewhat)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnUPED_SedGQ",
        "colab_type": "text"
      },
      "source": [
        "## Tensors that are used multiple times"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIf9R3Qqefml",
        "colab_type": "text"
      },
      "source": [
        "**The basic autograd has a rather pesky bug. Let’s squish it!**\n",
        "\n",
        "Sometimes, during forward propagation, you’ll use the same tensor multiple times (the weights of a neural network), and thus multiple parts of the graph will backpropagate gradients into the same tensor. But the code will currently compute the incorrect gradient when backpropagating into a variable that was used multiple times (is the parent of multiple children)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZqA4expfpKU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d6d40cb-fd2e-44c6-bdf3-f21dd587ff55"
      },
      "source": [
        "a = Tensor([1, 2, 3, 4, 5])\n",
        "b = Tensor([2, 2, 2, 2, 2])\n",
        "c = Tensor([5, 4, 3, 2, 1])\n",
        "\n",
        "d = a + b\n",
        "e = b + c\n",
        "f = d + e\n",
        "f.backward(Tensor(np.array([1, 1, 1, 1, 1])))\n",
        "print(b.grad.data == np.array([2, 2, 2, 2, 2]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[False False False False False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPYIvLSugZ3k",
        "colab_type": "text"
      },
      "source": [
        "In this example, the b variable is used twice in the process of creating f. Thus, its gradient should be the sum of two derivatives: [2,2,2,2,2]. Shown here is the resulting graph created by this chain of operations. Notice there are now two pointers pointing into b: so, it should be the sum of the gradient coming from both e and d.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/grokking-deep-learning/node-graph-2.png?raw=1' width='800'/>\n",
        "\n",
        "But the current implementation of Tensor merely overwrites each derivative with the previous. First, d applies its gradient, and then it gets overwritten with the gradient from e. We need to change the way gradients are written."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldXAmxE_grOt",
        "colab_type": "text"
      },
      "source": [
        "## Upgrading autograd to support multiuse tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_MCQ7LYgsQ1",
        "colab_type": "text"
      },
      "source": [
        "**Add one new function, and update three old ones.**\n",
        "\n",
        "This update to the Tensor object adds two new features. First, gradients can be accumulated so that when a variable is used more than once, it receives gradients from all children:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wpDQzQJ9vAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tensor(object):\n",
        "\n",
        "  def __init__(self, data, creators=None, creation_op=None):\n",
        "    self.data = np.array(data)\n",
        "    self.creation_op = creation_op\n",
        "    self.creators = creators\n",
        "    self.grad = None\n",
        "\n",
        "  def backward(self, grad):\n",
        "    self.grad = grad\n",
        "\n",
        "    if (self.creation_op == 'add'):\n",
        "      self.creators[0].backward(grad)\n",
        "      self.creators[1].backward(grad)\n",
        "\n",
        "  def __add__(self, other):\n",
        "    return Tensor(self.data + other.data, creators=[self, other], creation_op='add')\n",
        "\n",
        "  def __repr__(self):\n",
        "    return str(self.data.__repr__())\n",
        "\n",
        "  def __str__(self):\n",
        "    return str(self.data.__str__())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}