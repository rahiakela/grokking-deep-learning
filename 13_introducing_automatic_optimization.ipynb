{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "13-introducing-automatic-optimization.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMPWicntZxMXqUXZg5JOoky",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/grokking-deep-learning/blob/13-introducing-automatic-optimization/13_introducing_automatic_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKQqFDW4zOyt",
        "colab_type": "text"
      },
      "source": [
        "# Introducing automatic optimization: let’s build a deep learning framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLTada_-zSvz",
        "colab_type": "text"
      },
      "source": [
        "It’s extremely important for you to know what’s going on under the hood of these frameworks by implementing algorithms yourself (from scratch in NumPy). But now we’re going to transition into using a framework, because the networks you’ll be training next—long shortterm memory networks (LSTMs)—are very complex, and NumPy code describing their implementation is difficult to read, use, or debug (gradients are flying everywhere).\n",
        "\n",
        "It’s exactly this code complexity that deep learning frameworks were created to mitigate. **Especially if you wish to train a neural network on a GPU (giving 10–100× faster training), a deep learning framework can significantly reduce code complexity (reducing errors and increasing development speed) while also increasing runtime performance.**\n",
        "\n",
        "For these reasons, their use is nearly universal within the research community, and a thorough understanding of a deep learning framework will be essential on your journey toward becoming a user or researcher of deep learning.\n",
        "\n",
        "This way, you’ll have no doubt about what frameworks do when\n",
        "using them for complex architectures. Furthermore, building a small framework yourself should provide a smooth transition to using actual deep learning frameworks, because you’ll already be familiar with the API and the functionality underneath it.\n",
        "\n",
        "**Abstractly, it eliminates the need to write code that you’d repeat multiple times. Concretely, the most beneficial pieces of a deep learning\n",
        "framework are its support for automatic backpropagation and automatic optimization. These features let you specify only the forward propagation code of a model, with the framework taking care of backpropagation and weight updates automatically. Most frameworks even make the forward propagation code easier by providing high-level interfaces to common layers and loss functions.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pESZcC270o7u",
        "colab_type": "text"
      },
      "source": [
        "## Introduction to tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19_UwhuJ0rVY",
        "colab_type": "text"
      },
      "source": [
        "**Tensors are an abstract form of vectors and matrices.**\n",
        "\n",
        "Up to this point, we’ve been working exclusively with vectors and matrices as the basic data structures for deep learning. Recall that **a matrix is a list of vectors, and a vector is a list of scalars (single numbers). A tensor is the abstract version of this form of nested lists of numbers. A vector is a one-dimensional tensor. A matrix is a two-dimensional tensor, and higher dimensions are referred to as n-dimensional tensors**. Thus, the beginning of a new deep learning framework is the construction of this basic type, which we’ll call Tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk3Jf_761adO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oitoULz51h29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tensor(object):\n",
        "\n",
        "  def __init__(self, data):\n",
        "    self.data = np.array(data)\n",
        "\n",
        "  def __add__(self, other):\n",
        "    return Tensor(self.data + other.data)\n",
        "\n",
        "  def __repr__(self):\n",
        "    return str(self.data.__repr__())\n",
        "\n",
        "  def __str__(self):\n",
        "    return str(self.data.__str__())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0KAP7MN2qhA",
        "colab_type": "code",
        "outputId": "f4e5c969-84c8-4879-8f28-91a8acde4be3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = Tensor([1, 2, 3, 4, 5])\n",
        "print(x)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 3 4 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzkwYvuj2xOv",
        "colab_type": "code",
        "outputId": "91cc7b29-5a9f-47d0-94a5-4e3600e0e1f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y = x + x\n",
        "print(y)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2  4  6  8 10]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFDZRYa_3Dgw",
        "colab_type": "text"
      },
      "source": [
        "Note that it stores all the numerical information in a NumPy array (self.data), and it supports one tensor operation (addition). Adding more operations is relatively simple: create more functions on the tensor class with the appropriate functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlM152Vy3IzB",
        "colab_type": "text"
      },
      "source": [
        "## Introduction to automatic gradient computation(autograd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz7bMd2G3dUq",
        "colab_type": "text"
      },
      "source": [
        "**Stop! you performed backpropagation by hand.\n",
        "Let’s make it automatic!**\n",
        "\n",
        "You learned about derivatives. Since then, you’ve been computing derivatives\n",
        "by hand for each neural network you train. Recall that this is done by moving backward through the neural network: **first compute the gradient at the output of the network, then use that result to compute the derivative at the next-to-last component, and so on until all weights in the architecture have correct gradients. This logic for computing gradients can also be added to the tensor object.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiTFVteg22WC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tensor(object):\n",
        "\n",
        "  def __init__(self, data, creators=None, creation_op=None):\n",
        "    self.data = np.array(data)\n",
        "    self.creation_op = creation_op\n",
        "    self.creators = creators\n",
        "    self.grad = None\n",
        "\n",
        "  def backward(self, grad):\n",
        "    self.grad = grad\n",
        "\n",
        "    if (self.creation_op == 'add'):\n",
        "      self.creators[0].backward(grad)\n",
        "      self.creators[1].backward(grad)\n",
        "\n",
        "  def __add__(self, other):\n",
        "    return Tensor(self.data + other.data, creators=[self, other], creation_op='add')\n",
        "\n",
        "  def __repr__(self):\n",
        "    return str(self.data.__repr__())\n",
        "\n",
        "  def __str__(self):\n",
        "    return str(self.data.__str__())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQXKSrCb5EgH",
        "colab_type": "code",
        "outputId": "d77a4041-2a56-4622-c229-b950ed555f0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = Tensor([1, 2, 3, 4, 5])\n",
        "y = Tensor([2, 2, 2, 2, 2])\n",
        "print(x, y)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 3 4 5] [2 2 2 2 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LORe848Y5Wl6",
        "colab_type": "code",
        "outputId": "6f1a9c15-aa3a-4dfd-ce56-039f203c671f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "z = x + y\n",
        "print(z)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3 4 5 6 7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzsn5Yv85hA4",
        "colab_type": "code",
        "outputId": "43fb8f4e-3234-4de0-d948-9fc6eb10d339",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(z.backward(Tensor(np.array([1, 1, 1, 1, 1]))))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sU9HY6D_5yzI",
        "colab_type": "text"
      },
      "source": [
        "This method introduces two new concepts. First, each tensor gets two new attributes. creators is a list containing any tensors used in the creation of the current tensor (which defaults to None). Thus, when the two tensors $x$ and $y$ are added together, $z$ has two creators, $x$ and $y$. creation_op is a related feature that stores the instructions creators used in the creation process. \n",
        "\n",
        "**Thus, performing $z = x + y$ creates a computation graph with\n",
        "three nodes ($x$, $y$, and $z$) and two edges ($z -> x$ and $z -> y$). Each edge is labeled by the creation_op add. This graph allows you to recursively backpropagate gradients.**\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/grokking-deep-learning/node-graph.png?raw=1' width='800'/>\n",
        "\n",
        "**The first new concept in this implementation is the automatic creation of this graph whenever you perform math operations. If you took $z$ and performed further operations, the graph would continue with whatever resulting new variables pointed back to $z$.**\n",
        "\n",
        "**The second new concept introduced in this version of Tensor is the ability to use this graph to compute gradients.** When you call $z.backward()$, it sends the correct gradient for $x$ and $y$ given the function that was applied to create $z(add)$.\n",
        "\n",
        "Looking at the graph, you place a vector of gradients (np.array([1,1,1,1,1])) on $z$, and then they’re applied to their parents. As you know, backpropagating through addition means also applying addition when backpropagating. \n",
        "\n",
        "In this case, because there’s only one gradient to add into $x$\n",
        "or $y$, you copy the gradient from $z$ onto $x$ and $y$:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2UNGYKN8Yzm",
        "colab_type": "code",
        "outputId": "e038035b-c00e-474a-f6cb-c60d21e53fe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(x.grad)\n",
        "print(y.grad)\n",
        "print(z.creators)\n",
        "print(z.creation_op)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 1 1]\n",
            "[1 1 1 1 1]\n",
            "[array([1, 2, 3, 4, 5]), array([2, 2, 2, 2, 2])]\n",
            "add\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rptYWXUI87M1",
        "colab_type": "text"
      },
      "source": [
        "Perhaps the most elegant part of this form of autograd is that it works recursively as well, because each vector calls .backward() on all of its self.creators:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QffF0Qjh8wH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = Tensor([1, 2, 3, 4, 5])\n",
        "b = Tensor([2, 2, 2, 2, 2])\n",
        "c = Tensor([5, 4, 3, 2, 1])\n",
        "d = Tensor([-1, -2, -3, -4, -5])\n",
        "\n",
        "e = a + b\n",
        "f = c + d\n",
        "g = e + f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbLPtaW09mGO",
        "colab_type": "code",
        "outputId": "7f317836-9c8b-4a08-8346-889852299c4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "g.backward(Tensor(np.array([1, 1, 1, 1, 1])))\n",
        "print(a.grad)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmummaq9xqG",
        "colab_type": "text"
      },
      "source": [
        "## A quick checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCwqcHgS9yhx",
        "colab_type": "text"
      },
      "source": [
        "**Everything in Tensor is another form of lessons already learned.**\n",
        "\n",
        "Before moving on, I want to first acknowledge that even if it feels like a bit of a stretch or a heavy lift to think about gradients flowing over a graphical structure, this is nothing new compared to what you’ve already been working with. In the previous chapter on RNNs, you forward propagated in one direction and then back propagated across a (virtual graph) of activations.\n",
        "\n",
        "In particular, this notion of a graph that gets built during forward propagation is called a dynamic computation graph because it’s built\n",
        "on the fly during forward prop. This is the type of autograd present in newer deep learning frameworks such as DyNet and PyTorch. Older frameworks such as Theano and TensorFlow have what’s called a static computation graph, which is specified before forward propagation even begins.\n",
        "\n",
        "In general, dynamic computation graphs are easier to write/experiment with, and static computation graphs have faster runtimes because of some fancy logic under the hood. But note that dynamic and static frameworks have lately been moving toward the middle, allowing dynamic graphs to compile to static ones (for faster runtimes) or allowing static graphs to be built dynamically (for easier experimentation). In the long run, you’re likely to end up with both. The primary difference is whether forward propagation is happening during graph construction or after the graph is already defined.\n",
        "\n",
        "The main point, here, is to help prepare you for deep learning in the real world, where 10% (or less) of your time will be spent thinking up a new idea and 90% of your time will be spent figuring out how to get a deep learning framework to play nicely. Debugging these frameworks can be extremely difficult at times, because most bugs don’t raise an error and print out a stack trace. Most bugs lie hidden within the code, keeping the network from training as it should (even if it appears to be training somewhat)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnUPED_SedGQ",
        "colab_type": "text"
      },
      "source": [
        "## Tensors that are used multiple times"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIf9R3Qqefml",
        "colab_type": "text"
      },
      "source": [
        "**The basic autograd has a rather pesky bug. Let’s squish it!**\n",
        "\n",
        "Sometimes, during forward propagation, you’ll use the same tensor multiple times (the weights of a neural network), and thus multiple parts of the graph will backpropagate gradients into the same tensor. But the code will currently compute the incorrect gradient when backpropagating into a variable that was used multiple times (is the parent of multiple children)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZqA4expfpKU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "eb82edbb-dc4a-40e0-8821-642ef14c4d09"
      },
      "source": [
        "a = Tensor([1, 2, 3, 4, 5])\n",
        "b = Tensor([2, 2, 2, 2, 2])\n",
        "c = Tensor([5, 4, 3, 2, 1])\n",
        "\n",
        "d = a + b\n",
        "e = b + c\n",
        "f = d + e\n",
        "f.backward(Tensor(np.array([1, 1, 1, 1, 1])))\n",
        "print(b.grad.data == np.array([2, 2, 2, 2, 2]))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[False False False False False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPYIvLSugZ3k",
        "colab_type": "text"
      },
      "source": [
        "In this example, the b variable is used twice in the process of creating f. Thus, its gradient should be the sum of two derivatives: [2,2,2,2,2]. Shown here is the resulting graph created by this chain of operations. Notice there are now two pointers pointing into b: so, it should be the sum of the gradient coming from both e and d.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/grokking-deep-learning/node-graph-2.png?raw=1' width='800'/>\n",
        "\n",
        "But the current implementation of Tensor merely overwrites each derivative with the previous. First, d applies its gradient, and then it gets overwritten with the gradient from e. We need to change the way gradients are written."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldXAmxE_grOt",
        "colab_type": "text"
      },
      "source": [
        "## Upgrading autograd to support multiuse tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_MCQ7LYgsQ1",
        "colab_type": "text"
      },
      "source": [
        "**Add one new function, and update three old ones.**\n",
        "\n",
        "This update to the Tensor object adds two new features. First, gradients can be accumulated so that when a variable is used more than once, it receives gradients from all children:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wpDQzQJ9vAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tensor(object):\n",
        "\n",
        "  def __init__(self, data, autograd=False, creators=None, creation_op=None, id=None):\n",
        "    self.data = np.array(data)\n",
        "    self.creation_op = creation_op\n",
        "    self.creators = creators\n",
        "    self.grad = None\n",
        "    self.autograd = autograd\n",
        "    self.children = {}\n",
        "\n",
        "    if (id is None):\n",
        "      id = np.random.randint(0, 100000)\n",
        "    self.id = id\n",
        "\n",
        "    if (creators is not None):\n",
        "      for c in creators:\n",
        "        # Keeps track of how many children a tensor has\n",
        "        if (self.id not in c.children):\n",
        "          c.children[self.id] = 1\n",
        "        else:\n",
        "          c.children[self.id] += 1\n",
        "\n",
        "  def all_children_grads_accounted_for(self):\n",
        "    # Checks whether a tensor has received the correct number of gradients from each child\n",
        "    for id, cnt in self.children.items():\n",
        "      if(cnt != 0):\n",
        "        return False\n",
        "      return True\n",
        "\n",
        "  '''\n",
        "  Notice that addition isn’t handled anywhere else in the class. The generic backpropagation\n",
        "  logic is abstracted away so everything necessary for addition is defined in these two places.\n",
        "  Note further that backpropagation logic calls .backward() two times, once for each variable\n",
        "  that participated in the addition. Thus, the default setting in the backpropagation logic is to\n",
        "  always backpropagate into every variable in the graph.\n",
        "  '''\n",
        "  def backward(self, grad, grad_origin=None):\n",
        "    if (self.autograd):\n",
        "      if (grad_origin is not None):\n",
        "        # Checks to make sure you can backpropagate or whether you’re waiting for a gradient, in which case decrement the counter\n",
        "        if (self.children[grad_origin.id] == 0):\n",
        "          raise Exception('cannot backprop more than once')\n",
        "        else:\n",
        "          self.children[grad_origin.id] -= 1\n",
        "\n",
        "      # Accumulates gradients from several children\n",
        "      if (self.grad is None):\n",
        "        self.grad = grad\n",
        "      else:\n",
        "        self.grad += grad\n",
        "\n",
        "      if (self.creators is not None and (self.all_children_grads_accounted_for() or grad_origin is None)):\n",
        "        if (self.creation_op == 'add'):  # Begins actual backpropagation\n",
        "          self.creators[0].backward(self.grad, self)\n",
        "          self.creators[1].backward(self.grad, self)\n",
        "\n",
        "  def __add__(self, other):\n",
        "    if (self.autograd and other.autograd):\n",
        "      return Tensor(self.data + other.data, autograd=True, creators=[self, other], creation_op='add')\n",
        "    else:\n",
        "      return Tensor(self.data + other.data)\n",
        "\n",
        "  def __repr__(self):\n",
        "    return str(self.data.__repr__())\n",
        "\n",
        "  def __str__(self):\n",
        "    return str(self.data.__str__())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dV6MX4IF9Btn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d19c849d-ea52-4d60-99d3-a0e4c96e01f2"
      },
      "source": [
        "a = Tensor([1, 2, 3, 4, 5], autograd=True)\n",
        "b = Tensor([2, 2, 2, 2, 2], autograd=True)\n",
        "c = Tensor([5, 4, 3, 2, 1], autograd=True)\n",
        "\n",
        "d = a + b\n",
        "e = b + c\n",
        "f = d + e\n",
        "f.backward(Tensor(np.array([1, 1, 1, 1, 1])))\n",
        "print(b.grad.data == np.array([2, 2, 2, 2, 2]))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ True  True  True  True  True]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Guzaf85B_A6E",
        "colab_type": "text"
      },
      "source": [
        "Additionally, you create a self.children counter that counts the number of gradients received from each child during backpropagation. This way, you also prevent a variable from accidentally backpropagating from the same child twice (which throws an exception).\n",
        "\n",
        "Additionally, you create a self.children counter that counts the number of gradients received from each child during backpropagation. This way, you also prevent a variable from accidentally backpropagating from the same child twice (which throws an exception).\n",
        "\n",
        "As mentioned previously, none of these concepts are new from a deep learning theory perspective; these are the kinds of engineering challenges that deep learning frameworks seek to face. More important, they’re the kinds of challenges you’ll face when debugging neural networks in a standard framework. Before moving on, take a moment to play around and get familiar with this code.\n",
        "\n",
        "Try deleting different parts and seeing how it breaks in various ways. \n",
        "Try calling .backprop() twice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tty6xI169Ksw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "9073fad7-cc2f-4c12-f385-0a36ddd68984"
      },
      "source": [
        "a = Tensor([1, 2, 3, 4, 5], autograd=True)\n",
        "b = Tensor([2, 2, 2, 2, 2], autograd=True)\n",
        "c = Tensor([5, 4, 3, 2, 1], autograd=True)\n",
        "\n",
        "d = a + b\n",
        "e = b + c\n",
        "f = d + e\n",
        "f.backward(Tensor(np.array([1, 1, 1, 1, 1])))\n",
        "print(b.grad.data == np.array([2, 2, 2, 2, 2]))\n",
        "\n",
        "f.backward(Tensor(np.array([1, 1, 1, 1, 1])))\n",
        "print(b.grad.data == np.array([2, 2, 2, 2, 2]))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ True  True  True  True  True]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-ff89fd7bb34e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-a46aae197646>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     52\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_children_grads_accounted_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mgrad_origin\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'add'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Begins actual backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-a46aae197646>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Checks to make sure you can backpropagate or whether you’re waiting for a gradient, in which case decrement the counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrad_origin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cannot backprop more than once'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrad_origin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: cannot backprop more than once"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIAvBdDeA4-l",
        "colab_type": "text"
      },
      "source": [
        "## Adding support for negation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkoIyWl3A6IN",
        "colab_type": "text"
      },
      "source": [
        "**Let’s modify the support for addition to support negation.**\n",
        "\n",
        "Now that addition is working, you should be able to copy and paste the addition code, create a few modifications, and add autograd support for negation. \n",
        "\n",
        "Nearly everything is identical. You don’t accept any parameters so the parameter “other” has been removed in several places.\n",
        "\n",
        "Because the __neg__ function has only one creator, you end up calling .backward() only once.\n",
        "\n",
        "Let’s try it. Modifications from the __add__ function are added:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKnC1EntAInE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tensor(object):\n",
        "\n",
        "  def __init__(self, data, autograd=False, creators=None, creation_op=None, id=None):\n",
        "    self.data = np.array(data)\n",
        "    self.creation_op = creation_op\n",
        "    self.creators = creators\n",
        "    self.grad = None\n",
        "    self.autograd = autograd\n",
        "    self.children = {}\n",
        "\n",
        "    if (id is None):\n",
        "      id = np.random.randint(0, 100000)\n",
        "    self.id = id\n",
        "\n",
        "    if (creators is not None):\n",
        "      for c in creators:\n",
        "        # Keeps track of how many children a tensor has\n",
        "        if (self.id not in c.children):\n",
        "          c.children[self.id] = 1\n",
        "        else:\n",
        "          c.children[self.id] += 1\n",
        "\n",
        "  def all_children_grads_accounted_for(self):\n",
        "    # Checks whether a tensor has received the correct number of gradients from each child\n",
        "    for id, cnt in self.children.items():\n",
        "      if(cnt != 0):\n",
        "        return False\n",
        "      return True\n",
        "\n",
        "  '''\n",
        "  Notice that addition isn’t handled anywhere else in the class. The generic backpropagation\n",
        "  logic is abstracted away so everything necessary for addition is defined in these two places.\n",
        "  Note further that backpropagation logic calls .backward() two times, once for each variable\n",
        "  that participated in the addition. Thus, the default setting in the backpropagation logic is to\n",
        "  always backpropagate into every variable in the graph.\n",
        "  '''\n",
        "  def backward(self, grad, grad_origin=None):\n",
        "    if (self.autograd):\n",
        "      if (grad_origin is not None):\n",
        "        # Checks to make sure you can backpropagate or whether you’re waiting for a gradient, in which case decrement the counter\n",
        "        if (self.children[grad_origin.id] == 0):\n",
        "          raise Exception('cannot backprop more than once')\n",
        "        else:\n",
        "          self.children[grad_origin.id] -= 1\n",
        "\n",
        "      # Accumulates gradients from several children\n",
        "      if (self.grad is None):\n",
        "        self.grad = grad\n",
        "      else:\n",
        "        self.grad += grad\n",
        "\n",
        "      if (self.creators is not None and (self.all_children_grads_accounted_for() or grad_origin is None)):\n",
        "        if (self.creation_op == 'add'):  # Begins actual backpropagation for addition \n",
        "          self.creators[0].backward(self.grad, self)\n",
        "          self.creators[1].backward(self.grad, self)\n",
        "        if (self.creation_op == 'neg'):  # Begins actual backpropagation for negation\n",
        "          self.creators[0].backward(self.grad.__neg__())\n",
        "\n",
        "  def __add__(self, other):\n",
        "    if (self.autograd and other.autograd):\n",
        "      return Tensor(self.data + other.data, autograd=True, creators=[self, other], creation_op='add')\n",
        "    else:\n",
        "      return Tensor(self.data + other.data)\n",
        "\n",
        "  def __neg__(self):\n",
        "    if (self.autograd):\n",
        "      return Tensor(self.data * -1, autograd=True, creators=[self], creation_op='neg')\n",
        "    else:\n",
        "      return Tensor(self.data * -1)\n",
        "\n",
        "  def __repr__(self):\n",
        "    return str(self.data.__repr__())\n",
        "\n",
        "  def __str__(self):\n",
        "    return str(self.data.__str__())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hyz4HutqDo3A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e4f91c0b-725d-446b-d7ca-e676c29788ec"
      },
      "source": [
        "a = Tensor([1, 2, 3, 4, 5], autograd=True)\n",
        "b = Tensor([2, 2, 2, 2, 2], autograd=True)\n",
        "c = Tensor([5, 4, 3, 2, 1], autograd=True)\n",
        "\n",
        "d = a + (-b)\n",
        "e = (-b) + c\n",
        "f = d + e\n",
        "f.backward(Tensor(np.array([1, 1, 1, 1, 1])))\n",
        "print(b.grad.data == np.array([-2, -2, -2, -2, -2]))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ True  True  True  True  True]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayiiyJsTFMmm",
        "colab_type": "text"
      },
      "source": [
        "When you forward propagate using -b instead of b, the gradients that are backpropagated have a flipped sign as well. Furthermore, you don’t have to change anything about the general backpropagation system to make this work. You can create new functions as you need them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hekse_FSFQ1P",
        "colab_type": "text"
      },
      "source": [
        "## Adding support for additional functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3mTFBquFTgQ",
        "colab_type": "text"
      },
      "source": [
        "**Subtraction, multiplication, sum, expand, transpose, and matrix multiplication**"
      ]
    }
  ]
}