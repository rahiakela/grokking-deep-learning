{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "13-introducing-automatic-optimization.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPPBd//gbGea3Kp5QOQalO9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/grokking-deep-learning/blob/13-introducing-automatic-optimization/13_introducing_automatic_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKQqFDW4zOyt",
        "colab_type": "text"
      },
      "source": [
        "# Introducing automatic optimization: let’s build a deep learning framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLTada_-zSvz",
        "colab_type": "text"
      },
      "source": [
        "It’s extremely important for you to know what’s going on under the hood of these frameworks by implementing algorithms yourself (from scratch in NumPy). But now we’re going to transition into using a framework, because the networks you’ll be training next—long shortterm memory networks (LSTMs)—are very complex, and NumPy code describing their implementation is difficult to read, use, or debug (gradients are flying everywhere).\n",
        "\n",
        "It’s exactly this code complexity that deep learning frameworks were created to mitigate. **Especially if you wish to train a neural network on a GPU (giving 10–100× faster training), a deep learning framework can significantly reduce code complexity (reducing errors and increasing development speed) while also increasing runtime performance.**\n",
        "\n",
        "For these reasons, their use is nearly universal within the research community, and a thorough understanding of a deep learning framework will be essential on your journey toward becoming a user or researcher of deep learning.\n",
        "\n",
        "This way, you’ll have no doubt about what frameworks do when\n",
        "using them for complex architectures. Furthermore, building a small framework yourself should provide a smooth transition to using actual deep learning frameworks, because you’ll already be familiar with the API and the functionality underneath it.\n",
        "\n",
        "**Abstractly, it eliminates the need to write code that you’d repeat multiple times. Concretely, the most beneficial pieces of a deep learning\n",
        "framework are its support for automatic backpropagation and automatic optimization. These features let you specify only the forward propagation code of a model, with the framework taking care of backpropagation and weight updates automatically. Most frameworks even make the forward propagation code easier by providing high-level interfaces to common layers and loss functions.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pESZcC270o7u",
        "colab_type": "text"
      },
      "source": [
        "## Introduction to tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19_UwhuJ0rVY",
        "colab_type": "text"
      },
      "source": [
        "**Tensors are an abstract form of vectors and matrices.**\n",
        "\n",
        "Up to this point, we’ve been working exclusively with vectors and matrices as the basic data structures for deep learning. Recall that **a matrix is a list of vectors, and a vector is a list of scalars (single numbers). A tensor is the abstract version of this form of nested lists of numbers. A vector is a one-dimensional tensor. A matrix is a two-dimensional tensor, and higher dimensions are referred to as n-dimensional tensors**. Thus, the beginning of a new deep learning framework is the construction of this basic type, which we’ll call Tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk3Jf_761adO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oitoULz51h29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tensor(object):\n",
        "\n",
        "  def __init__(self, data):\n",
        "    self.data = np.array(data)\n",
        "\n",
        "  def __add__(self, other):\n",
        "    return Tensor(self.data + other.data)\n",
        "\n",
        "  def __repr__(self):\n",
        "    return str(self.data.__repr__())\n",
        "\n",
        "  def __str__(self):\n",
        "    return str(self.data.__str__())"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0KAP7MN2qhA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "c29bdd8c-d6ef-4cd4-b7f6-e88a1e69ac39"
      },
      "source": [
        "x = Tensor([1, 2, 3, 4, 5])\n",
        "print(x)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 3 4 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzkwYvuj2xOv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "904bb373-e47c-4de2-a063-a5aa1f614ea7"
      },
      "source": [
        "y = x + x\n",
        "print(y)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2  4  6  8 10]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFDZRYa_3Dgw",
        "colab_type": "text"
      },
      "source": [
        "Note that it stores all the numerical information in a NumPy array (self.data), and it supports one tensor operation (addition). Adding more operations is relatively simple: create more functions on the tensor class with the appropriate functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlM152Vy3IzB",
        "colab_type": "text"
      },
      "source": [
        "## Introduction to automatic gradient computation(autograd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz7bMd2G3dUq",
        "colab_type": "text"
      },
      "source": [
        "**Stop! you performed backpropagation by hand.\n",
        "Let’s make it automatic!**\n",
        "\n",
        "You learned about derivatives. Since then, you’ve been computing derivatives\n",
        "by hand for each neural network you train. Recall that this is done by moving backward through the neural network: **first compute the gradient at the output of the network, then use that result to compute the derivative at the next-to-last component, and so on until all weights in the architecture have correct gradients. This logic for computing gradients can also be added to the tensor object.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiTFVteg22WC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tensor(object):\n",
        "\n",
        "  def __init__(self, data, creators=None, creation_op=None):\n",
        "    self.data = np.array(data)\n",
        "    self.creation_op = creation_op\n",
        "    self.creators = creators\n",
        "    self.grad = None\n",
        "\n",
        "  def backward(self, grad):\n",
        "    self.grad = grad\n",
        "\n",
        "    if (self.creation_op == 'add'):\n",
        "      self.creators[0].backward(grad)\n",
        "      self.creators[1].backward(grad)\n",
        "\n",
        "  def __add__(self, other):\n",
        "    return Tensor(self.data + other.data, creators=[self, other], creation_op='add')\n",
        "\n",
        "  def __repr__(self):\n",
        "    return str(self.data.__repr__())\n",
        "\n",
        "  def __str__(self):\n",
        "    return str(self.data.__str__())"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQXKSrCb5EgH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d2d23063-effc-4dfe-a0fc-3eb166ff7172"
      },
      "source": [
        "x = Tensor([1, 2, 3, 4, 5])\n",
        "y = Tensor([2, 2, 2, 2, 2])\n",
        "print(x, y)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 3 4 5] [2 2 2 2 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LORe848Y5Wl6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "44754f34-58f4-405f-bc32-81a95a6c2355"
      },
      "source": [
        "z = x + y\n",
        "print(z)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3 4 5 6 7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzsn5Yv85hA4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "094b4da4-be14-4441-974c-dcae0037b87c"
      },
      "source": [
        "print(z.backward(Tensor(np.array([1, 1, 1, 1, 1]))))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sU9HY6D_5yzI",
        "colab_type": "text"
      },
      "source": [
        "This method introduces two new concepts. First, each tensor gets two new attributes. creators is a list containing any tensors used in the creation of the current tensor (which defaults to None). Thus, when the two tensors $x$ and $y$ are added together, $z$ has two creators, $x$ and $y$. creation_op is a related feature that stores the instructions creators used in the creation process. \n",
        "\n",
        "**Thus, performing $z = x + y$ creates a computation graph with\n",
        "three nodes ($x$, $y$, and $z$) and two edges ($z -> x$ and $z -> y$). Each edge is labeled by the creation_op add. This graph allows you to recursively backpropagate gradients.**\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/grokking-deep-learning/node-graph.png?raw=1' width='800'/>\n",
        "\n",
        "**The first new concept in this implementation is the automatic creation of this graph whenever you perform math operations. If you took $z$ and performed further operations, the graph would continue with whatever resulting new variables pointed back to $z$.**\n",
        "\n",
        "**The second new concept introduced in this version of Tensor is the ability to use this graph to compute gradients.** When you call $z.backward()$, it sends the correct gradient for $x$ and $y$ given the function that was applied to create $z(add)$.\n",
        "\n",
        "Looking at the graph, you place a vector of gradients (np.array([1,1,1,1,1])) on $z$, and then they’re applied to their parents. As you know, backpropagating through addition means also applying addition when backpropagating. \n",
        "\n",
        "In this case, because there’s only one gradient to add into $x$\n",
        "or $y$, you copy the gradient from $z$ onto $x$ and $y$:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2UNGYKN8Yzm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "90492b2c-79aa-45d2-b5b2-9b850c952526"
      },
      "source": [
        "print(x.grad)\n",
        "print(y.grad)\n",
        "print(z.creators)\n",
        "print(z.creation_op)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 1 1]\n",
            "[1 1 1 1 1]\n",
            "[array([1, 2, 3, 4, 5]), array([2, 2, 2, 2, 2])]\n",
            "add\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rptYWXUI87M1",
        "colab_type": "text"
      },
      "source": [
        "Perhaps the most elegant part of this form of autograd is that it works recursively as well, because each vector calls .backward() on all of its self.creators:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QffF0Qjh8wH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = Tensor([1, 2, 3, 4, 5])\n",
        "b = Tensor([2, 2, 2, 2, 2])\n",
        "c = Tensor([5, 4, 3, 2, 1])\n",
        "d = Tensor([-1, -2, -3, -4, -5])\n",
        "\n",
        "e = a + b\n",
        "f = c + d\n",
        "g = e + f"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbLPtaW09mGO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b9b494c3-4849-4996-8222-b60ebf2e063c"
      },
      "source": [
        "g.backward(Tensor(np.array([1, 1, 1, 1, 1])))\n",
        "print(a.grad)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmummaq9xqG",
        "colab_type": "text"
      },
      "source": [
        "## A quick checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCwqcHgS9yhx",
        "colab_type": "text"
      },
      "source": [
        "**Everything in Tensor is another form of lessons already learned.**\n",
        "\n",
        "Before moving on, I want to first acknowledge that even if it feels like a bit of a stretch or a heavy lift to think about gradients flowing over a graphical structure, this is nothing new compared to what you’ve already been working with. In the previous chapter on RNNs, you forward propagated in one direction and then back propagated across a (virtual graph) of activations.\n",
        "\n",
        "In particular, this notion of a graph that gets built during forward propagation is called a dynamic computation graph because it’s built\n",
        "on the fly during forward prop. This is the type of autograd present in newer deep learning frameworks such as DyNet and PyTorch. Older frameworks such as Theano and TensorFlow have what’s called a static computation graph, which is specified before forward propagation even begins.\n",
        "\n",
        "In general, dynamic computation graphs are easier to write/experiment with, and static computation graphs have faster runtimes because of some fancy logic under the hood. But note that dynamic and static frameworks have lately been moving toward the middle, allowing dynamic graphs to compile to static ones (for faster runtimes) or allowing static graphs to be built dynamically (for easier experimentation). In the long run, you’re likely to end up with both. The primary difference is whether forward propagation is happening during graph construction or after the graph is already defined.\n",
        "\n",
        "The main point, here, is to help prepare you for deep learning in the real world, where 10% (or less) of your time will be spent thinking up a new idea and 90% of your time will be spent figuring out how to get a deep learning framework to play nicely. Debugging these frameworks can be extremely difficult at times, because most bugs don’t raise an error and print out a stack trace. Most bugs lie hidden within the code, keeping the network from training as it should (even if it appears to be training somewhat)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnUPED_SedGQ",
        "colab_type": "text"
      },
      "source": [
        "## Tensors that are used multiple times"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIf9R3Qqefml",
        "colab_type": "text"
      },
      "source": [
        "**The basic autograd has a rather pesky bug. Let’s squish it!**\n",
        "\n",
        "Sometimes, during forward propagation, you’ll use the same tensor multiple times (the weights of a neural network), and thus multiple parts of the graph will backpropagate gradients into the same tensor. But the code will currently compute the incorrect gradient when backpropagating into a variable that was used multiple times (is the parent of multiple children)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZqA4expfpKU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "12fd20da-3c7b-485e-cdad-f30ae9c9b241"
      },
      "source": [
        "a = Tensor([1, 2, 3, 4, 5])\n",
        "b = Tensor([2, 2, 2, 2, 2])\n",
        "c = Tensor([5, 4, 3, 2, 1])\n",
        "\n",
        "d = a + b\n",
        "e = b + c\n",
        "f = d + e\n",
        "f.backward(Tensor(np.array([1, 1, 1, 1, 1])))\n",
        "print(b.grad.data == np.array([2, 2, 2, 2, 2]))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[False False False False False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPYIvLSugZ3k",
        "colab_type": "text"
      },
      "source": [
        "In this example, the b variable is used twice in the process of creating f. Thus, its gradient should be the sum of two derivatives: [2,2,2,2,2]. Shown here is the resulting graph created by this chain of operations. Notice there are now two pointers pointing into b: so, it should be the sum of the gradient coming from both e and d.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/grokking-deep-learning/node-graph-2.png?raw=1' width='800'/>\n",
        "\n",
        "But the current implementation of Tensor merely overwrites each derivative with the previous. First, d applies its gradient, and then it gets overwritten with the gradient from e. We need to change the way gradients are written."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldXAmxE_grOt",
        "colab_type": "text"
      },
      "source": [
        "## Upgrading autograd to support multiuse tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_MCQ7LYgsQ1",
        "colab_type": "text"
      },
      "source": [
        "**Add one new function, and update three old ones.**\n",
        "\n",
        "This update to the Tensor object adds two new features. First, gradients can be accumulated so that when a variable is used more than once, it receives gradients from all children:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wpDQzQJ9vAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tensor(object):\n",
        "\n",
        "  def __init__(self, data, autograd=False, creators=None, creation_op=None, id=None):\n",
        "    self.data = np.array(data)\n",
        "    self.creation_op = creation_op\n",
        "    self.creators = creators\n",
        "    self.grad = None\n",
        "    self.autograd = autograd\n",
        "    self.children = {}\n",
        "\n",
        "    if (id is None):\n",
        "      id = np.random.randint(0, 100000)\n",
        "    self.id = id\n",
        "\n",
        "    if (creators is not None):\n",
        "      for c in creators:\n",
        "        # Keeps track of how many children a tensor has\n",
        "        if (self.id not in c.children):\n",
        "          c.children[self.id] = 1\n",
        "        else:\n",
        "          c.children[self.id] += 1\n",
        "\n",
        "  def all_children_grads_accounted_for(self):\n",
        "    # Checks whether a tensor has received the correct number of gradients from each child\n",
        "    for id, cnt in self.children.items():\n",
        "      if(cnt != 0):\n",
        "        return False\n",
        "      return True\n",
        "\n",
        "  '''\n",
        "  Notice that addition isn’t handled anywhere else in the class. The generic backpropagation\n",
        "  logic is abstracted away so everything necessary for addition is defined in these two places.\n",
        "  Note further that backpropagation logic calls .backward() two times, once for each variable\n",
        "  that participated in the addition. Thus, the default setting in the backpropagation logic is to\n",
        "  always backpropagate into every variable in the graph.\n",
        "  '''\n",
        "  def backward(self, grad, grad_origin=None):\n",
        "    if (self.autograd):\n",
        "      if (grad_origin is not None):\n",
        "        # Checks to make sure you can backpropagate or whether you’re waiting for a gradient, in which case decrement the counter\n",
        "        if (self.children[grad_origin.id] == 0):\n",
        "          raise Exception('cannot backprop more than once')\n",
        "        else:\n",
        "          self.children[grad_origin.id] -= 1\n",
        "\n",
        "      # Accumulates gradients from several children\n",
        "      if (self.grad is None):\n",
        "        self.grad = grad\n",
        "      else:\n",
        "        self.grad += grad\n",
        "\n",
        "      if (self.creators is not None and (self.all_children_grads_accounted_for() or grad_origin is None)):\n",
        "        if (self.creation_op == 'add'):  # Begins actual backpropagation\n",
        "          self.creators[0].backward(self.grad, self)\n",
        "          self.creators[1].backward(self.grad, self)\n",
        "\n",
        "  def __add__(self, other):\n",
        "    if (self.autograd and other.autograd):\n",
        "      return Tensor(self.data + other.data, autograd=True, creators=[self, other], creation_op='add')\n",
        "    else:\n",
        "      return Tensor(self.data + other.data)\n",
        "\n",
        "  def __repr__(self):\n",
        "    return str(self.data.__repr__())\n",
        "\n",
        "  def __str__(self):\n",
        "    return str(self.data.__str__())"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dV6MX4IF9Btn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "913c3d25-0705-4caa-9969-aaa5de10a601"
      },
      "source": [
        "a = Tensor([1, 2, 3, 4, 5], autograd=True)\n",
        "b = Tensor([2, 2, 2, 2, 2], autograd=True)\n",
        "c = Tensor([5, 4, 3, 2, 1], autograd=True)\n",
        "\n",
        "d = a + b\n",
        "e = b + c\n",
        "f = d + e\n",
        "f.backward(Tensor(np.array([1, 1, 1, 1, 1])))\n",
        "print(b.grad.data == np.array([2, 2, 2, 2, 2]))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ True  True  True  True  True]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Guzaf85B_A6E",
        "colab_type": "text"
      },
      "source": [
        "Additionally, you create a self.children counter that counts the number of gradients received from each child during backpropagation. This way, you also prevent a variable from accidentally backpropagating from the same child twice (which throws an exception).\n",
        "\n",
        "Additionally, you create a self.children counter that counts the number of gradients received from each child during backpropagation. This way, you also prevent a variable from accidentally backpropagating from the same child twice (which throws an exception).\n",
        "\n",
        "As mentioned previously, none of these concepts are new from a deep learning theory perspective; these are the kinds of engineering challenges that deep learning frameworks seek to face. More important, they’re the kinds of challenges you’ll face when debugging neural networks in a standard framework. Before moving on, take a moment to play around and get familiar with this code.\n",
        "\n",
        "Try deleting different parts and seeing how it breaks in various ways. \n",
        "Try calling .backprop() twice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tty6xI169Ksw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8de8d8a9-b419-47f5-872a-93f6e130068f"
      },
      "source": [
        "a = Tensor([1, 2, 3, 4, 5], autograd=True)\n",
        "b = Tensor([2, 2, 2, 2, 2], autograd=True)\n",
        "c = Tensor([5, 4, 3, 2, 1], autograd=True)\n",
        "\n",
        "d = a + b\n",
        "e = b + c\n",
        "f = d + e\n",
        "f.backward(Tensor(np.array([1, 1, 1, 1, 1])))\n",
        "print(b.grad.data == np.array([2, 2, 2, 2, 2]))\n",
        "\n",
        "# f.backward(Tensor(np.array([1, 1, 1, 1, 1])))\n",
        "# print(b.grad.data == np.array([2, 2, 2, 2, 2]))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ True  True  True  True  True]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIAvBdDeA4-l",
        "colab_type": "text"
      },
      "source": [
        "## Adding support for negation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkoIyWl3A6IN",
        "colab_type": "text"
      },
      "source": [
        "**Let’s modify the support for addition to support negation.**\n",
        "\n",
        "Now that addition is working, you should be able to copy and paste the addition code, create a few modifications, and add autograd support for negation. \n",
        "\n",
        "Nearly everything is identical. You don’t accept any parameters so the parameter “other” has been removed in several places.\n",
        "\n",
        "Because the __neg__ function has only one creator, you end up calling .backward() only once.\n",
        "\n",
        "Let’s try it. Modifications from the __add__ function are added:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKnC1EntAInE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tensor(object):\n",
        "\n",
        "  def __init__(self, data, autograd=False, creators=None, creation_op=None, id=None):\n",
        "    self.data = np.array(data)\n",
        "    self.creation_op = creation_op\n",
        "    self.creators = creators\n",
        "    self.grad = None\n",
        "    self.autograd = autograd\n",
        "    self.children = {}\n",
        "\n",
        "    if (id is None):\n",
        "      id = np.random.randint(0, 100000)\n",
        "    self.id = id\n",
        "\n",
        "    if (creators is not None):\n",
        "      for c in creators:\n",
        "        # Keeps track of how many children a tensor has\n",
        "        if (self.id not in c.children):\n",
        "          c.children[self.id] = 1\n",
        "        else:\n",
        "          c.children[self.id] += 1\n",
        "\n",
        "  def all_children_grads_accounted_for(self):\n",
        "    # Checks whether a tensor has received the correct number of gradients from each child\n",
        "    for id, cnt in self.children.items():\n",
        "      if(cnt != 0):\n",
        "        return False\n",
        "      return True\n",
        "\n",
        "  '''\n",
        "  Notice that addition isn’t handled anywhere else in the class. The generic backpropagation\n",
        "  logic is abstracted away so everything necessary for addition is defined in these two places.\n",
        "  Note further that backpropagation logic calls .backward() two times, once for each variable\n",
        "  that participated in the addition. Thus, the default setting in the backpropagation logic is to\n",
        "  always backpropagate into every variable in the graph.\n",
        "  '''\n",
        "  def backward(self, grad, grad_origin=None):\n",
        "    if (self.autograd):\n",
        "      if (grad_origin is not None):\n",
        "        # Checks to make sure you can backpropagate or whether you’re waiting for a gradient, in which case decrement the counter\n",
        "        if (self.children[grad_origin.id] == 0):\n",
        "          raise Exception('cannot backprop more than once')\n",
        "        else:\n",
        "          self.children[grad_origin.id] -= 1\n",
        "\n",
        "      # Accumulates gradients from several children\n",
        "      if (self.grad is None):\n",
        "        self.grad = grad\n",
        "      else:\n",
        "        self.grad += grad\n",
        "\n",
        "      if (self.creators is not None and (self.all_children_grads_accounted_for() or grad_origin is None)):\n",
        "        if (self.creation_op == 'add'):  # Begins actual backpropagation for addition \n",
        "          self.creators[0].backward(self.grad, self)\n",
        "          self.creators[1].backward(self.grad, self)\n",
        "        if (self.creation_op == 'neg'):  # Begins actual backpropagation for negation\n",
        "          self.creators[0].backward(self.grad.__neg__())\n",
        "\n",
        "  def __add__(self, other):\n",
        "    if (self.autograd and other.autograd):\n",
        "      return Tensor(self.data + other.data, autograd=True, creators=[self, other], creation_op='add')\n",
        "    else:\n",
        "      return Tensor(self.data + other.data)\n",
        "\n",
        "  def __neg__(self):\n",
        "    if (self.autograd):\n",
        "      return Tensor(self.data * -1, autograd=True, creators=[self], creation_op='neg')\n",
        "    else:\n",
        "      return Tensor(self.data * -1)\n",
        "\n",
        "  def __repr__(self):\n",
        "    return str(self.data.__repr__())\n",
        "\n",
        "  def __str__(self):\n",
        "    return str(self.data.__str__())"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hyz4HutqDo3A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a5aec4b1-c0fc-42e7-f14c-5c4f206c3b20"
      },
      "source": [
        "a = Tensor([1, 2, 3, 4, 5], autograd=True)\n",
        "b = Tensor([2, 2, 2, 2, 2], autograd=True)\n",
        "c = Tensor([5, 4, 3, 2, 1], autograd=True)\n",
        "\n",
        "d = a + (-b)\n",
        "e = (-b) + c\n",
        "f = d + e\n",
        "f.backward(Tensor(np.array([1, 1, 1, 1, 1])))\n",
        "print(b.grad.data == np.array([-2, -2, -2, -2, -2]))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ True  True  True  True  True]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayiiyJsTFMmm",
        "colab_type": "text"
      },
      "source": [
        "When you forward propagate using -b instead of b, the gradients that are backpropagated have a flipped sign as well. Furthermore, you don’t have to change anything about the general backpropagation system to make this work. You can create new functions as you need them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hekse_FSFQ1P",
        "colab_type": "text"
      },
      "source": [
        "## Adding support for additional functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3mTFBquFTgQ",
        "colab_type": "text"
      },
      "source": [
        "**Subtraction, multiplication, sum, expand, transpose, and matrix multiplication**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP_h_9G-GC0e",
        "colab_type": "text"
      },
      "source": [
        "Using the same ideas you learned for addition and negation, let’s add the forward and backpropagation logic for several additional functions and You can now add the corresponding backpropagation logic to the .backward() method::"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKOqC2ggGHOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tensor(object):\n",
        "\n",
        "  def __init__(self, data, autograd=False, creators=None, creation_op=None, id=None):\n",
        "    self.data = np.array(data)\n",
        "    self.creation_op = creation_op\n",
        "    self.creators = creators\n",
        "    self.grad = None\n",
        "    self.autograd = autograd\n",
        "    self.children = {}\n",
        "\n",
        "    if (id is None):\n",
        "      id = np.random.randint(0, 100000)\n",
        "    else:\n",
        "      self.id = id\n",
        "\n",
        "    if (creators is not None):\n",
        "      for c in creators:\n",
        "        # Keeps track of how many children a tensor has\n",
        "        if (self.id not in c.children):\n",
        "          c.children[self.id] = 1\n",
        "        else:\n",
        "          c.children[self.id] += 1\n",
        "\n",
        "  def all_children_grads_accounted_for(self):\n",
        "    # Checks whether a tensor has received the correct number of gradients from each child\n",
        "    for id, cnt in self.children.items():\n",
        "      if(cnt != 0):\n",
        "        return False\n",
        "      return True\n",
        "\n",
        "  '''\n",
        "  Notice that addition isn’t handled anywhere else in the class. The generic backpropagation\n",
        "  logic is abstracted away so everything necessary for addition is defined in these two places.\n",
        "  Note further that backpropagation logic calls .backward() two times, once for each variable\n",
        "  that participated in the addition. Thus, the default setting in the backpropagation logic is to\n",
        "  always backpropagate into every variable in the graph.\n",
        "  '''\n",
        "  def backward(self, grad, grad_origin=None):\n",
        "    if (self.autograd):\n",
        "\n",
        "      if (grad is None):\n",
        "        grad = Tensor(np.ones_like(self.data))\n",
        "\n",
        "      if (grad_origin is not None):\n",
        "        # Checks to make sure you can backpropagate or whether you’re waiting for a gradient, in which case decrement the counter\n",
        "        if (self.children[grad_origin.id] == 0):\n",
        "          raise Exception('cannot backprop more than once')\n",
        "        else:\n",
        "          self.children[grad_origin.id] -= 1\n",
        "\n",
        "      # Accumulates gradients from several children\n",
        "      if (self.grad is None):\n",
        "        self.grad = grad\n",
        "      else:\n",
        "        self.grad += grad\n",
        "\n",
        "      # grads must not have grads of their own\n",
        "      assert grad.autograd == False\n",
        "\n",
        "      if (self.creators is not None and (self.all_children_grads_accounted_for() or grad_origin is None)):\n",
        "        if (self.creation_op == 'add'):  # Begins actual backpropagation for addition \n",
        "          self.creators[0].backward(self.grad, self)\n",
        "          self.creators[1].backward(self.grad, self)\n",
        "        if (self.creation_op == 'neg'):  # Begins actual backpropagation for negation\n",
        "          self.creators[0].backward(self.grad.__neg__())\n",
        "        if (self.creation_op == 'sub'):  # Begins actual backpropagation for subtraction\n",
        "          new = Tensor(self.grad.data)\n",
        "          self.creators[0].backward(new, self)\n",
        "          new = Tensor(self.grad.__neg__().data)\n",
        "          self.creators[1].backward(new, self)\n",
        "        if (self.creation_op == 'mul'):  # Begins actual backpropagation for multiplication\n",
        "          new = self.grad * self.creators[1]\n",
        "          self.creators[0].backward(new, self)\n",
        "          new = self.grad * self.creators[0]\n",
        "          self.creators[1].backward(new, self)\n",
        "        if (self.creation_op == 'mm'):  # Begins actual backpropagation for matrix multiplication\n",
        "          activation = self.creators[0] # Usually an activation\n",
        "          weights = self.creators[1]    # Usually an weight matrix\n",
        "          new = self.grad.mm(weights.transpose())\n",
        "          activation.backward(new)\n",
        "          new = self.grad.transpose().mm(activation).transpose()\n",
        "          weights.backward(new)\n",
        "        if (self.creation_op == 'transpose'):  # Begins actual backpropagation for transpose\n",
        "          self.creators[0].backward(self.grad.transpose())\n",
        "        if ('sum' in self.creation_op):  # Begins actual backpropagation for sum\n",
        "          dim = int(self.creation_op.split('_')[1])\n",
        "          ds = self.creators[0].data.shape[dim]\n",
        "          self.creators[0].backward(self.grad.expand(dim, ds))\n",
        "        if ('expand' in self.creation_op):  # Begins actual backpropagation for expand\n",
        "          dim = int(self.creation_op.split('_')[1])\n",
        "          self.creators[0].backward(self.grad.sum(dim))\n",
        "\n",
        "  def __add__(self, other):\n",
        "    if (self.autograd and other.autograd):\n",
        "      return Tensor(self.data + other.data, autograd=True, creators=[self, other], creation_op='add')\n",
        "    else:\n",
        "      return Tensor(self.data + other.data)\n",
        "\n",
        "  def __neg__(self):\n",
        "    if (self.autograd):\n",
        "      return Tensor(self.data * -1, autograd=True, creators=[self], creation_op='neg')\n",
        "    else:\n",
        "      return Tensor(self.data * -1)\n",
        "  \n",
        "  def __sub__(self, other):\n",
        "    if (self.autograd and other.autograd):\n",
        "      return Tensor(self.data - other.data, autograd=True, creators=[self, other], creation_op='sub')\n",
        "    return Tensor(self.data - other.data)\n",
        "\n",
        "  def __mul__(self, other):\n",
        "    if (self.autograd and other.autograd):\n",
        "      return Tensor(self.data * other.data, autograd=True, creators=[self, other], creation_op='mul')\n",
        "    return Tensor(self.data * other.data)\n",
        "\n",
        "  def sum(self, dim):\n",
        "    if (self.autograd):\n",
        "      return Tensor(self.data.sum(dim), autograd=True, creators=[self], creation_op='sum_' + str(dim))\n",
        "    return Tensor(self.data.sum(dim))\n",
        "\n",
        "  def expand(self, dim, copies):\n",
        "    trans_cmd = list(range(0, len(self.data.shape)))\n",
        "    trans_cmd.insert(dim, len(self.data.shape))\n",
        "\n",
        "    #new_shape = list(self.data.shape) + [copies]\n",
        "    #new_data = self.data.repeat(copies).reshape(new_shape)\n",
        "    #new_data = new_data.transpose(trans_cmd)\n",
        "    new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
        "\n",
        "    if (self.autograd):\n",
        "      return Tensor(new_data, autograd=True, creators=[self], creation_op='expand_' + str(dim))\n",
        "    return Tensor(new_data)\n",
        "\n",
        "  def transpose(self):\n",
        "    if (self.autograd):\n",
        "      return Tensor(self.data.transpose(), autograd=True, creators=[self], creation_op='transpose')\n",
        "    return Tensor(self.data.transpose())\n",
        "\n",
        "  def mm(self, x):\n",
        "    if (self.autograd):\n",
        "      return Tensor(self.data.dot(x.data), autograd=True, creators=[self, x], creation_op='mm')\n",
        "    return Tensor(self.data.dot(x.data))\n",
        "\n",
        "  def __repr__(self):\n",
        "    return str(self.data.__repr__())\n",
        "\n",
        "  def __str__(self):\n",
        "    return str(self.data.__str__())"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9ZFsml9OzPG",
        "colab_type": "text"
      },
      "source": [
        "We’ve previously discussed the derivatives for all these functions, although sum and expand might seem foreign because they have new names. sum performs addition across a dimension of the tensor; in other words, say you have a 2 × 3 matrix called x:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCRUF4PVKCr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = Tensor(np.array([\n",
        "  [1, 2, 3],\n",
        "  [4, 5, 6]                \n",
        "]))"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5ZrK-JLPFlp",
        "colab_type": "text"
      },
      "source": [
        "The .sum(dim) function sums across a dimension.\n",
        "\n",
        "Where x.sum(0) will result in a 1 × 3 matrix (a length 3 vector), summing columns values.\n",
        "\n",
        "whereas x.sum(1) will result in a 2 × 1 matrix (a length 2 vector), summing rows values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zZ5dnM0PB97",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "af9e80c9-47d7-40e7-fd4f-e93b59e2123b"
      },
      "source": [
        "x.sum(0) "
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 7, 9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hslw9ZPiPKHV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9b3fd8fc-7e7f-4a89-c8e1-80bff047f5ef"
      },
      "source": [
        "x.sum(1)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 6, 15])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRUrggfyP3SH",
        "colab_type": "text"
      },
      "source": [
        "You use expand to backpropagate through a .sum(). It’s a function that copies data along a dimension. \n",
        "\n",
        "Given the same matrix x, copying along the first dimension gives two copies of the tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATXBPjbXPMJz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "66d80fe5-9a21-4608-95e9-c9a745ec2f14"
      },
      "source": [
        "x.expand(dim=0, copies=4)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[1, 2, 3],\n",
              "        [4, 5, 6]],\n",
              "\n",
              "       [[1, 2, 3],\n",
              "        [4, 5, 6]],\n",
              "\n",
              "       [[1, 2, 3],\n",
              "        [4, 5, 6]],\n",
              "\n",
              "       [[1, 2, 3],\n",
              "        [4, 5, 6]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhVSWBuHQrC7",
        "colab_type": "text"
      },
      "source": [
        "To be clear, whereas .sum() removes a dimension (2 × 3 -> just 2 or 3), expand adds a dimension. The 2 × 3 matrix becomes 4 × 2 × 3. You can think of this as a list of four tensors, each of which is 2 × 3. \n",
        "\n",
        "But if you expand to the last dimension, it copies along the last dimension, so each entry in the original tensor becomes a list of entries instead:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us5DX8O5QKkZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "5cb538fd-67c1-4550-9212-ef597dcc7a3b"
      },
      "source": [
        "x.expand(dim=2, copies=4)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[1, 1, 1, 1],\n",
              "        [2, 2, 2, 2],\n",
              "        [3, 3, 3, 3]],\n",
              "\n",
              "       [[4, 4, 4, 4],\n",
              "        [5, 5, 5, 5],\n",
              "        [6, 6, 6, 6]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDGo8HUzRBcz",
        "colab_type": "text"
      },
      "source": [
        "Thus, when you perform .sum(dim=1) on a tensor with four entries in that dimension, you need to perform .expand(dim=1, copies=4) to the gradient when you backpropagate it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51RugXJSQ0rt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "1ce75bd1-e591-415c-eda7-3cb207061c9d"
      },
      "source": [
        "x.expand(dim=1, copies=4)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[1, 2, 3],\n",
              "        [1, 2, 3],\n",
              "        [1, 2, 3],\n",
              "        [1, 2, 3]],\n",
              "\n",
              "       [[4, 5, 6],\n",
              "        [4, 5, 6],\n",
              "        [4, 5, 6],\n",
              "        [4, 5, 6]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc94a2gFRZpY",
        "colab_type": "text"
      },
      "source": [
        "The gradients start at the end of the network. You then move the error signal backward through the network by calling functions that correspond to the functions used to move activations forward through the network. If the last operation was a matrix multiplication (and it was), you backpropagate by performing matrix multiplication (dot) on the transposed matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OpeHrIBR0tu",
        "colab_type": "text"
      },
      "source": [
        "## Using autograd to train a neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF1zkvKbR1sU",
        "colab_type": "text"
      },
      "source": [
        "**You no longer have to write backpropagation logic!**\n",
        "\n",
        "Now, when you train a neural network, you don’t have to write any backpropagation logic! \n",
        "\n",
        "\n",
        "As a toy example, here’s a neural network to backprop by hand:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjv8YRqvRGUX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "f40d29f3-9a49-47be-b9fc-d68b4fd469b7"
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "data = np.array([\n",
        "   [0, 0],\n",
        "   [0, 1],\n",
        "   [1, 0],\n",
        "   [1, 1]              \n",
        "])\n",
        "target = np.array([\n",
        "   [0],\n",
        "   [1],\n",
        "   [0],\n",
        "   [1]                \n",
        "])\n",
        "\n",
        "weights_0_1 = np.random.rand(2, 3)\n",
        "weights_0_2 = np.random.rand(3, 1)\n",
        "\n",
        "for i in range(10):\n",
        "  # Predict\n",
        "  layer_1 = data.dot(weights_0_1)\n",
        "  layer_2 = layer_1.dot(weights_0_2)\n",
        "\n",
        "  # Compare\n",
        "  diff = (layer_2 - target)\n",
        "  sqdiff = (diff * diff)\n",
        "  loss = sqdiff.sum(0)\n",
        "\n",
        "  # Learn; this is the backpropagation piece\n",
        "  layer_1_grad = diff.dot(weights_0_2.transpose())\n",
        "  weights_0_2_update = layer_1.transpose().dot(diff)\n",
        "  weights_0_1_update = data.transpose().dot(layer_1_grad)\n",
        "\n",
        "  weights_0_2 -= weights_0_2_update * 0.1\n",
        "  weights_0_1 -= weights_0_1_update * 0.1\n",
        "  print(loss[0])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4953698041458336\n",
            "1.3535697560859723\n",
            "1.0983854696420685\n",
            "0.9402318232494947\n",
            "0.8218819800278834\n",
            "0.7268126358071565\n",
            "0.6475517024114271\n",
            "0.5798445170397148\n",
            "0.5209684792243789\n",
            "0.46906878966910576\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8to8oRXSzV1",
        "colab_type": "text"
      },
      "source": [
        "You have to forward propagate in such a way that layer_1, layer_2, and diff exist as variables, because you need them later. You then have to backpropagate each gradient to its appropriate weight matrix and perform the weight update appropriately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ2h9FzeSkHz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "a3a20a4d-e98a-4f0a-957d-34e9e8e76d1f"
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "data = Tensor(np.array([\n",
        "   [0, 0],\n",
        "   [0, 1],\n",
        "   [1, 0],\n",
        "   [1, 1]                     \n",
        "]), autograd=True)\n",
        "\n",
        "target = Tensor(np.array([\n",
        "   [0],\n",
        "   [1],\n",
        "   [0],\n",
        "   [1]                     \n",
        "]), autograd=True)\n",
        "\n",
        "w = list()\n",
        "w.append(Tensor(np.random.rand(2, 3), autograd=True))\n",
        "w.append(Tensor(np.random.rand(3, 1), autograd=True))\n",
        "\n",
        "for i in range(10):\n",
        "  # Predict\n",
        "  pred = data.mm(w[0]).mm(w[1])\n",
        "\n",
        "  # Compare\n",
        "  loss = ((pred - target) * (pred - target)).sum(0)\n",
        "\n",
        "  # Learn; this is the backpropagation piece\n",
        "  loss.backward(Tensor(np.ones_like(loss.data)))\n",
        "\n",
        "  for w_ in w:\n",
        "    w_.data -= w_.grad.data * 0.1\n",
        "    w_.grad.data *= 0\n",
        "  \n",
        "  print(loss)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.11374267]\n",
            "[0.62255894]\n",
            "[0.3880155]\n",
            "[0.2383683]\n",
            "[0.13717701]\n",
            "[0.07307696]\n",
            "[0.03598853]\n",
            "[0.01689497]\n",
            "[0.00830702]\n",
            "[0.00459155]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8DYx0ctVIAj",
        "colab_type": "text"
      },
      "source": [
        "But with the fancy new autograd system, the code is much simpler. You don’t have to keep around any temporary variables (because the dynamic graph keeps track of them), and you don’t have to implement any backpropagation logic (because the .backward() method handles that). Not only is this more convenient, but you’re less likely to make silly mistakes in the backpropagation code, reducing the likelihood of bugs!\n",
        "\n",
        "Notice that I put all the parameters in a list, which I could iterate through when performing the weight update. This is a bit of foreshadowing for the next piece of functionality. When you have an autograd system, stochastic gradient descent becomes trivial to implement (it’s just that for loop at the end). Let’s try making this its own class as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prLaqmvtZmUC",
        "colab_type": "text"
      },
      "source": [
        "## Final Tensor Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBKvk9w5ZrQo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tensor (object):\n",
        "    \n",
        "    def __init__(self,data,\n",
        "                 autograd=False,\n",
        "                 creators=None,\n",
        "                 creation_op=None,\n",
        "                 id=None):\n",
        "        \n",
        "        self.data = np.array(data)\n",
        "        self.autograd = autograd\n",
        "        self.grad = None\n",
        "        if(id is None):\n",
        "            self.id = np.random.randint(0,100000)\n",
        "        else:\n",
        "            self.id = id\n",
        "        \n",
        "        self.creators = creators\n",
        "        self.creation_op = creation_op\n",
        "        self.children = {}\n",
        "        \n",
        "        if(creators is not None):\n",
        "            for c in creators:\n",
        "                if(self.id not in c.children):\n",
        "                    c.children[self.id] = 1\n",
        "                else:\n",
        "                    c.children[self.id] += 1\n",
        "\n",
        "    def all_children_grads_accounted_for(self):\n",
        "        for id,cnt in self.children.items():\n",
        "            if(cnt != 0):\n",
        "                return False\n",
        "        return True \n",
        "        \n",
        "    def backward(self,grad=None, grad_origin=None):\n",
        "        if(self.autograd):\n",
        " \n",
        "            if(grad is None):\n",
        "                grad = Tensor(np.ones_like(self.data))\n",
        "\n",
        "            if(grad_origin is not None):\n",
        "                if(self.children[grad_origin.id] == 0):\n",
        "                    raise Exception(\"cannot backprop more than once\")\n",
        "                else:\n",
        "                    self.children[grad_origin.id] -= 1\n",
        "\n",
        "            if(self.grad is None):\n",
        "                self.grad = grad\n",
        "            else:\n",
        "                self.grad += grad\n",
        "            \n",
        "            # grads must not have grads of their own\n",
        "            assert grad.autograd == False\n",
        "            \n",
        "            # only continue backpropping if there's something to\n",
        "            # backprop into and if all gradients (from children)\n",
        "            # are accounted for override waiting for children if\n",
        "            # \"backprop\" was called on this variable directly\n",
        "            if(self.creators is not None and \n",
        "               (self.all_children_grads_accounted_for() or \n",
        "                grad_origin is None)):\n",
        "\n",
        "                if(self.creation_op == \"add\"):\n",
        "                    self.creators[0].backward(self.grad, self)\n",
        "                    self.creators[1].backward(self.grad, self)\n",
        "                    \n",
        "                if(self.creation_op == \"sub\"):\n",
        "                    self.creators[0].backward(Tensor(self.grad.data), self)\n",
        "                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)\n",
        "\n",
        "                if(self.creation_op == \"mul\"):\n",
        "                    new = self.grad * self.creators[1]\n",
        "                    self.creators[0].backward(new , self)\n",
        "                    new = self.grad * self.creators[0]\n",
        "                    self.creators[1].backward(new, self)                    \n",
        "                    \n",
        "                if(self.creation_op == \"mm\"):\n",
        "                    c0 = self.creators[0]\n",
        "                    c1 = self.creators[1]\n",
        "                    new = self.grad.mm(c1.transpose())\n",
        "                    c0.backward(new)\n",
        "                    new = self.grad.transpose().mm(c0).transpose()\n",
        "                    c1.backward(new)\n",
        "                    \n",
        "                if(self.creation_op == \"transpose\"):\n",
        "                    self.creators[0].backward(self.grad.transpose())\n",
        "\n",
        "                if(\"sum\" in self.creation_op):\n",
        "                    dim = int(self.creation_op.split(\"_\")[1])\n",
        "                    self.creators[0].backward(self.grad.expand(dim,\n",
        "                                                               self.creators[0].data.shape[dim]))\n",
        "\n",
        "                if(\"expand\" in self.creation_op):\n",
        "                    dim = int(self.creation_op.split(\"_\")[1])\n",
        "                    self.creators[0].backward(self.grad.sum(dim))\n",
        "                    \n",
        "                if(self.creation_op == \"neg\"):\n",
        "                    self.creators[0].backward(self.grad.__neg__())\n",
        "                    \n",
        "    def __add__(self, other):\n",
        "        if(self.autograd and other.autograd):\n",
        "            return Tensor(self.data + other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"add\")\n",
        "        return Tensor(self.data + other.data)\n",
        "\n",
        "    def __neg__(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data * -1,\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"neg\")\n",
        "        return Tensor(self.data * -1)\n",
        "    \n",
        "    def __sub__(self, other):\n",
        "        if(self.autograd and other.autograd):\n",
        "            return Tensor(self.data - other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"sub\")\n",
        "        return Tensor(self.data - other.data)\n",
        "    \n",
        "    def __mul__(self, other):\n",
        "        if(self.autograd and other.autograd):\n",
        "            return Tensor(self.data * other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"mul\")\n",
        "        return Tensor(self.data * other.data)    \n",
        "\n",
        "    def sum(self, dim):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data.sum(dim),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"sum_\"+str(dim))\n",
        "        return Tensor(self.data.sum(dim))\n",
        "    \n",
        "    def expand(self, dim,copies):\n",
        "\n",
        "        trans_cmd = list(range(0,len(self.data.shape)))\n",
        "        trans_cmd.insert(dim,len(self.data.shape))\n",
        "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
        "        \n",
        "        if(self.autograd):\n",
        "            return Tensor(new_data,\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"expand_\"+str(dim))\n",
        "        return Tensor(new_data)\n",
        "    \n",
        "    def transpose(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data.transpose(),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"transpose\")\n",
        "        \n",
        "        return Tensor(self.data.transpose())\n",
        "    \n",
        "    def mm(self, x):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data.dot(x.data),\n",
        "                          autograd=True,\n",
        "                          creators=[self,x],\n",
        "                          creation_op=\"mm\")\n",
        "        return Tensor(self.data.dot(x.data))\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return str(self.data.__repr__())\n",
        "    \n",
        "    def __str__(self):\n",
        "        return str(self.data.__str__()) "
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOwPDwN6VgXz",
        "colab_type": "text"
      },
      "source": [
        "## Adding automatic optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1v3ZbuLArOG",
        "colab_type": "text"
      },
      "source": [
        "**Let’s make a stochastic gradient descent optimizer.**\n",
        "\n",
        "At face value, creating something called a stochastic gradient descent optimizer may sound difficult, but it’s just copying and pasting from the previous example with a bit of good, oldfashioned object-oriented programming:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr_YUZeMUW4v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SGD(object):\n",
        "\n",
        "  def __init__(self, parameters, alpha=0.1):\n",
        "    self.parameters = parameters\n",
        "    self.alpha = alpha\n",
        "\n",
        "  def zero(self):\n",
        "    for p in self.parameters:\n",
        "      p.grad.data *= 0\n",
        "\n",
        "  def optimize(self, zero=True):\n",
        "    for p in self.parameters:\n",
        "      p.data -= p.grad.data * self.alpha\n",
        "      if (zero):\n",
        "        p.grad.data *= 0 "
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVf7tQa0B5s9",
        "colab_type": "text"
      },
      "source": [
        "The previous neural network is further simplified as follows, with exactly the same results as before:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vux9-1wxB5G_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "8cc3cf78-cccf-4db7-afce-70bc7e124724"
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "data = Tensor(np.array([\n",
        "   [0, 0],\n",
        "   [0, 1],\n",
        "   [1, 0],\n",
        "   [1, 1]                     \n",
        "]), autograd=True)\n",
        "\n",
        "target = Tensor(np.array([\n",
        "   [0],\n",
        "   [1],\n",
        "   [0],\n",
        "   [1]                     \n",
        "]), autograd=True)\n",
        "\n",
        "w = list()\n",
        "w.append(Tensor(np.random.rand(2, 3), autograd=True))\n",
        "w.append(Tensor(np.random.rand(3, 1), autograd=True))\n",
        "\n",
        "sgd = SGD(parameters=w, alpha=0.1)\n",
        "\n",
        "for i in range(10):\n",
        "  # Predict\n",
        "  pred = data.mm(w[0]).mm(w[1])\n",
        "\n",
        "  # Compare\n",
        "  loss = ((pred - target) * (pred - target)).sum(0)\n",
        "\n",
        "  # Learn; this is the backpropagation piece\n",
        "  loss.backward(Tensor(np.ones_like(loss.data)))\n",
        "\n",
        "  ######### Replace it with SGD #############\n",
        "  # for w_ in w:\n",
        "  #  w_.data -= w_.grad.data * 0.1\n",
        "  #  w_.grad.data *= 0\n",
        "  \n",
        "  # optimize using SGD\n",
        "  sgd.optimize()\n",
        "  \n",
        "  print(loss)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.11374267]\n",
            "[0.62255894]\n",
            "[0.3880155]\n",
            "[0.2383683]\n",
            "[0.13717701]\n",
            "[0.07307696]\n",
            "[0.03598853]\n",
            "[0.01689497]\n",
            "[0.00830702]\n",
            "[0.00459155]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raNwVfuMCoER",
        "colab_type": "text"
      },
      "source": [
        "## Adding support for layer types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x72r16PtCpJl",
        "colab_type": "text"
      },
      "source": [
        "**You may be familiar with layer types in Keras or PyTorch.**\n",
        "\n",
        "Probably the most common abstraction among nearly all frameworks is the layer abstraction. It’s a collection of commonly used forward propagation techniques packaged into an simple API with some kind of .forward() method to call them.\n",
        "\n",
        "Here’s an example of a simple linear layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_PPy4PjB4AP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.parameters = list()\n",
        "\n",
        "  def get_parameters(self):\n",
        "    return self.parameters\n",
        "\n",
        "class Linear(Layer):\n",
        "\n",
        "  def __init__(self, n_inputs, n_outputs):\n",
        "    super().__init__()\n",
        "    # weight and bias initilization\n",
        "    W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0 / (n_inputs))\n",
        "    self.weight = Tensor(W, autograd=True)\n",
        "    self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
        "\n",
        "    self.parameters.append(self.weight)\n",
        "    self.parameters.append(self.bias)\n",
        "\n",
        "  def forward(self, input):\n",
        "    return input.mm(self.weight) + self.bias.expand(0, len(input.data))"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIyiPw6oFfBc",
        "colab_type": "text"
      },
      "source": [
        "Nothing here is particularly new. The weights are organized into a class (and I added bias weights because this is a true linear layer). You can initialize the layer all together, such that both the weights and bias are initialized with the correct sizes, and the correct forward propagation logic is always employed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8r2VeCkGNlp",
        "colab_type": "text"
      },
      "source": [
        "## Layers that contain layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTmjXR9IGOYB",
        "colab_type": "text"
      },
      "source": [
        "**Layers can also contain other layers.**\n",
        "\n",
        "The most popular layer is a sequential layer that forward propagates a list of layers, where each layer feeds its outputs into the inputs of the next layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBTdZoSFFeJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sequential(Layer):\n",
        "\n",
        "  def __init__(self, layers=list()):\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "\n",
        "  def add(self, layer):\n",
        "    self.layers.append(layer)\n",
        "\n",
        "  def forward(self, input):\n",
        "    for layer in self.layers:\n",
        "      input = layer.forward(input)\n",
        "    return input\n",
        "\n",
        "  def get_parameters(self):\n",
        "    params = list()\n",
        "    for l in self.layers:\n",
        "      params += l.get_parameters()\n",
        "    return params"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoySyL-sHjUW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "e4f0d71e-859e-4058-fc31-a1df10f48fb5"
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "data = Tensor(np.array([\n",
        "   [0, 0],\n",
        "   [0, 1],\n",
        "   [1, 0],\n",
        "   [1, 1]                     \n",
        "]), autograd=True)\n",
        "\n",
        "target = Tensor(np.array([\n",
        "   [0],\n",
        "   [1],\n",
        "   [0],\n",
        "   [1]                     \n",
        "]), autograd=True)\n",
        "\n",
        "######### Replace it with Sequential layer #############\n",
        "# w = list()\n",
        "# w.append(Tensor(np.random.rand(2, 3), autograd=True))\n",
        "# w.append(Tensor(np.random.rand(3, 1), autograd=True))\n",
        "\n",
        "model = Sequential([Linear(2, 3), Linear(3, 1)])\n",
        "sgd = SGD(parameters=model.get_parameters(), alpha=0.05)\n",
        "\n",
        "for i in range(10):\n",
        "  # Predict\n",
        "  pred = model.forward(data)\n",
        "\n",
        "  # Compare\n",
        "  loss = ((pred - target) * (pred - target)).sum(0)\n",
        "  \n",
        "  # Learn; this is the backpropagation piece\n",
        "  loss.backward(Tensor(np.ones_like(loss.data)))\n",
        "\n",
        "  # optimize using SGD\n",
        "  sgd.optimize()\n",
        "  \n",
        "  print(loss)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3.65094627]\n",
            "[5.24649682]\n",
            "[7.49768079]\n",
            "[2.58132095]\n",
            "[0.82653187]\n",
            "[0.26937059]\n",
            "[0.2301955]\n",
            "[0.19896456]\n",
            "[0.17283695]\n",
            "[0.15067618]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZICHYoqcMI3",
        "colab_type": "text"
      },
      "source": [
        "## Loss-function layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9s1qDo6cNDB",
        "colab_type": "text"
      },
      "source": [
        "**Some layers have no weights.**\n",
        "\n",
        "You can also create layers that are functions on the input. The most popular version of this kind of layer is probably the loss-function layer, such as mean squared error:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR23A4xUKhR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MSELoss(Layer):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, pred, target):\n",
        "    return ((pred - target) * (pred - target)).sum(0)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8gbz-WReB6s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "47248ce7-ea25-4f56-fd2a-28c668e3b404"
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "data = Tensor(np.array([\n",
        "   [0, 0],\n",
        "   [0, 1],\n",
        "   [1, 0],\n",
        "   [1, 1]                     \n",
        "]), autograd=True)\n",
        "\n",
        "target = Tensor(np.array([\n",
        "   [0],\n",
        "   [1],\n",
        "   [0],\n",
        "   [1]                     \n",
        "]), autograd=True)\n",
        "\n",
        "model = Sequential([Linear(2, 3), Linear(3, 1)])\n",
        "criterion = MSELoss()\n",
        "sgd = SGD(parameters=model.get_parameters(), alpha=0.05)\n",
        "\n",
        "for i in range(10):\n",
        "  # Predict\n",
        "  pred = model.forward(data)\n",
        "\n",
        "  ######### Replace it with MSELoss layer #############\n",
        "  # loss = ((pred - target) * (pred - target)).sum(0)\n",
        "  loss = criterion.forward(pred, target)\n",
        "\n",
        "  # Learn; this is the backpropagation piece\n",
        "  loss.backward(Tensor(np.ones_like(loss.data)))\n",
        "\n",
        "  # optimize using SGD\n",
        "  sgd.optimize()\n",
        "  \n",
        "  print(loss)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3.65094627]\n",
            "[5.24649682]\n",
            "[7.49768079]\n",
            "[2.58132095]\n",
            "[0.82653187]\n",
            "[0.26937059]\n",
            "[0.2301955]\n",
            "[0.19896456]\n",
            "[0.17283695]\n",
            "[0.15067618]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUXhNMeHfBkc",
        "colab_type": "text"
      },
      "source": [
        "If you’ll forgive the repetition, again, nothing here is particularly new. Under the hood, the last several code examples all do the exact same computation. It’s just that autograd is doing all the backpropagation, and the forward propagation steps are packaged in nice classes to ensure that the functionality executes in the correct order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1tKxBuDfGNy",
        "colab_type": "text"
      },
      "source": [
        "## Refectoring the backpropagation piece"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVKmQRCWerpD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SGD(object):\n",
        "\n",
        "  def __init__(self, parameters, alpha=0.1):\n",
        "    self.parameters = parameters\n",
        "    self.alpha = alpha\n",
        "\n",
        "  def zero(self):\n",
        "    for p in self.parameters:\n",
        "      p.grad.data *= 0\n",
        "\n",
        "  def optimize(self, zero=True, loss=None):\n",
        "    # Learn; this is the backpropagation piece\n",
        "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
        "    \n",
        "    for p in self.parameters:\n",
        "      p.data -= p.grad.data * self.alpha\n",
        "      if (zero):\n",
        "        p.grad.data *= 0 "
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-oQ6Mo5fp_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "0456714a-5d24-4973-8c60-0ed353d9e142"
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "data = Tensor(np.array([\n",
        "   [0, 0],\n",
        "   [0, 1],\n",
        "   [1, 0],\n",
        "   [1, 1]                     \n",
        "]), autograd=True)\n",
        "\n",
        "target = Tensor(np.array([\n",
        "   [0],\n",
        "   [1],\n",
        "   [0],\n",
        "   [1]                     \n",
        "]), autograd=True)\n",
        "\n",
        "model = Sequential([Linear(2, 3), Linear(3, 1)])\n",
        "criterion = MSELoss()\n",
        "sgd = SGD(parameters=model.get_parameters(), alpha=0.05)\n",
        "\n",
        "for i in range(10):\n",
        "  # Predict\n",
        "  pred = model.forward(data)\n",
        "\n",
        "  loss = criterion.forward(pred, target)\n",
        "\n",
        "  ######### Remove it, this piece of code moved to SGD #############\n",
        "  # Learn; this is the backpropagation piece\n",
        "  # loss.backward(Tensor(np.ones_like(loss.data)))\n",
        "\n",
        "  # optimize using SGD with loss\n",
        "  sgd.optimize(loss=loss)\n",
        "  \n",
        "  print(loss)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3.65094627]\n",
            "[5.24649682]\n",
            "[7.49768079]\n",
            "[2.58132095]\n",
            "[0.82653187]\n",
            "[0.26937059]\n",
            "[0.2301955]\n",
            "[0.19896456]\n",
            "[0.17283695]\n",
            "[0.15067618]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZls5Mk0gSx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}