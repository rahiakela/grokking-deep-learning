{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "13-introducing-automatic-optimization.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPNqeC+/Uzx8Ofrx7ZRJOcY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/grokking-deep-learning/blob/13-introducing-automatic-optimization/13_introducing_automatic_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKQqFDW4zOyt",
        "colab_type": "text"
      },
      "source": [
        "# Introducing automatic optimization: let’s build a deep learning framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLTada_-zSvz",
        "colab_type": "text"
      },
      "source": [
        "It’s extremely important for you to know what’s going on under the hood of these frameworks by implementing algorithms yourself (from scratch in NumPy). But now we’re going to transition into using a framework, because the networks you’ll be training next—long shortterm memory networks (LSTMs)—are very complex, and NumPy code describing their implementation is difficult to read, use, or debug (gradients are flying everywhere).\n",
        "\n",
        "It’s exactly this code complexity that deep learning frameworks were created to mitigate. **Especially if you wish to train a neural network on a GPU (giving 10–100× faster training), a deep learning framework can significantly reduce code complexity (reducing errors and increasing development speed) while also increasing runtime performance.**\n",
        "\n",
        "For these reasons, their use is nearly universal within the research community, and a thorough understanding of a deep learning framework will be essential on your journey toward becoming a user or researcher of deep learning.\n",
        "\n",
        "This way, you’ll have no doubt about what frameworks do when\n",
        "using them for complex architectures. Furthermore, building a small framework yourself should provide a smooth transition to using actual deep learning frameworks, because you’ll already be familiar with the API and the functionality underneath it.\n",
        "\n",
        "**Abstractly, it eliminates the need to write code that you’d repeat multiple times. Concretely, the most beneficial pieces of a deep learning\n",
        "framework are its support for automatic backpropagation and automatic optimization. These features let you specify only the forward propagation code of a model, with the framework taking care of backpropagation and weight updates automatically. Most frameworks even make the forward propagation code easier by providing high-level interfaces to common layers and loss functions.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pESZcC270o7u",
        "colab_type": "text"
      },
      "source": [
        "## Introduction to tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19_UwhuJ0rVY",
        "colab_type": "text"
      },
      "source": [
        "**Tensors are an abstract form of vectors and matrices.**\n",
        "\n",
        "Up to this point, we’ve been working exclusively with vectors and matrices as the basic data structures for deep learning. Recall that **a matrix is a list of vectors, and a vector is a list of scalars (single numbers). A tensor is the abstract version of this form of nested lists of numbers. A vector is a one-dimensional tensor. A matrix is a two-dimensional tensor, and higher dimensions are referred to as n-dimensional tensors**. Thus, the beginning of a new deep learning framework is the construction of this basic type, which we’ll call Tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk3Jf_761adO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oitoULz51h29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tensor(object):\n",
        "\n",
        "  def __init__(self, data):\n",
        "    self.data = np.array(data)\n",
        "\n",
        "  def __add__(self, other):\n",
        "    return Tensor(self.data + other.data)\n",
        "\n",
        "  def __repr__(self):\n",
        "    return str(self.data.__repr__())\n",
        "\n",
        "  def __str__(self):\n",
        "    return str(self.data.__str__())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0KAP7MN2qhA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "63c641cb-0c4f-4804-b7cd-1f74856d2f95"
      },
      "source": [
        "x = Tensor([1, 2, 3, 4, 5])\n",
        "print(x)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 3 4 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzkwYvuj2xOv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1ccf223e-4125-4b9e-8fae-61a255bd11f1"
      },
      "source": [
        "y = x + x\n",
        "print(y)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2  4  6  8 10]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFDZRYa_3Dgw",
        "colab_type": "text"
      },
      "source": [
        "Note that it stores all the numerical information in a NumPy array (self.data), and it supports one tensor operation (addition). Adding more operations is relatively simple: create more functions on the tensor class with the appropriate functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlM152Vy3IzB",
        "colab_type": "text"
      },
      "source": [
        "## Introduction to automatic gradient computation(autograd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz7bMd2G3dUq",
        "colab_type": "text"
      },
      "source": [
        "**Stop! you performed backpropagation by hand.\n",
        "Let’s make it automatic!**\n",
        "\n",
        "You learned about derivatives. Since then, you’ve been computing derivatives\n",
        "by hand for each neural network you train. Recall that this is done by moving backward through the neural network: **first compute the gradient at the output of the network, then use that result to compute the derivative at the next-to-last component, and so on until all weights in the architecture have correct gradients. This logic for computing gradients can also be added to the tensor object.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiTFVteg22WC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tensor(object):\n",
        "\n",
        "  def __init__(self, data, creators=None, creation_op=None):\n",
        "    self.data = np.array(data)\n",
        "    self.creation_op = creation_op\n",
        "    self.creators = creators\n",
        "    self.grad = None\n",
        "\n",
        "  def backward(self, grad):\n",
        "    self.grad = grad\n",
        "\n",
        "    if (self.creation_op == 'add'):\n",
        "      self.creators[0].backward(grad)\n",
        "      self.creators[1].backward(grad)\n",
        "\n",
        "  def __add__(self, other):\n",
        "    return Tensor(self.data + other.data, creators=[self, other], creation_op='add')\n",
        "\n",
        "  def __repr__(self):\n",
        "    return str(self.data.__repr__())\n",
        "\n",
        "  def __str__(self):\n",
        "    return str(self.data.__str__())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQXKSrCb5EgH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eed90f97-d4c6-405d-cb12-d2276a6236bd"
      },
      "source": [
        "x = Tensor([1, 2, 3, 4, 5])\n",
        "y = Tensor([2, 2, 2, 2, 2])\n",
        "print(x, y)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 3 4 5] [2 2 2 2 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LORe848Y5Wl6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "25820cad-1b96-4237-c61b-ae11701999b1"
      },
      "source": [
        "z = x + y\n",
        "print(z)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3 4 5 6 7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzsn5Yv85hA4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aeb593fe-285d-4084-b60a-e1059a30433a"
      },
      "source": [
        "print(z.backward(Tensor(np.array([1, 1, 1, 1, 1]))))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sU9HY6D_5yzI",
        "colab_type": "text"
      },
      "source": [
        "This method introduces two new concepts. First, each tensor gets two new attributes. creators is a list containing any tensors used in the creation of the current tensor (which defaults to None). Thus, when the two tensors $x$ and $y$ are added together, $z$ has two creators, $x$ and $y$. creation_op is a related feature that stores the instructions creators used in the creation process. \n",
        "\n",
        "**Thus, performing $z = x + y$ creates a computation graph with\n",
        "three nodes ($x$, $y$, and $z$) and two edges ($z -> x$ and $z -> y$). Each edge is labeled by the creation_op add. This graph allows you to recursively backpropagate gradients.**\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/grokking-deep-learning/node-graph.png?raw=1' width='800'/>\n",
        "\n",
        "**The first new concept in this implementation is the automatic creation of this graph whenever you perform math operations. If you took $z$ and performed further operations, the graph would continue with whatever resulting new variables pointed back to $z$.**\n",
        "\n",
        "**The second new concept introduced in this version of Tensor is the ability to use this graph to compute gradients.** When you call $z.backward()$, it sends the correct gradient for $x$ and $y$ given the function that was applied to create $z(add)$.\n",
        "\n",
        "Looking at the graph, you place a vector of gradients (np.array([1,1,1,1,1])) on $z$, and then they’re applied to their parents. As you know, backpropagating through addition means also applying addition when backpropagating. \n",
        "\n",
        "In this case, because there’s only one gradient to add into $x$\n",
        "or $y$, you copy the gradient from $z$ onto $x$ and $y$:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2UNGYKN8Yzm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ba477c67-21bd-44ff-85c9-8a35dd7c86ab"
      },
      "source": [
        "print(x.grad)\n",
        "print(y.grad)\n",
        "print(z.creators)\n",
        "print(z.creation_op)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 1 1]\n",
            "[1 1 1 1 1]\n",
            "[array([1, 2, 3, 4, 5]), array([2, 2, 2, 2, 2])]\n",
            "add\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rptYWXUI87M1",
        "colab_type": "text"
      },
      "source": [
        "Perhaps the most elegant part of this form of autograd is that it works recursively as well, because each vector calls .backward() on all of its self.creators:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QffF0Qjh8wH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = Tensor([1, 2, 3, 4, 5])\n",
        "b = Tensor([2, 2, 2, 2, 2])\n",
        "c = Tensor([5, 4, 3, 2, 1])\n",
        "d = Tensor([-1, -2, -3, -4, -5])\n",
        "\n",
        "e = a + b\n",
        "f = c + d\n",
        "g = e + f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbLPtaW09mGO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e81dba4f-ed6e-483e-e4b4-51eea3084e89"
      },
      "source": [
        "g.backward(Tensor(np.array([1, 1, 1, 1, 1])))\n",
        "print(a.grad)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmummaq9xqG",
        "colab_type": "text"
      },
      "source": [
        "## A quick checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCwqcHgS9yhx",
        "colab_type": "text"
      },
      "source": [
        "**Everything in Tensor is another form of lessons already learned.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wpDQzQJ9vAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}