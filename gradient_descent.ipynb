{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gradient-descent.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/grokking-deep-learning/blob/4-gradient-descent/gradient_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUDd4oT1agdf",
        "colab_type": "text"
      },
      "source": [
        "# introduction to neural learning: gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZB9oWbPEbGC_",
        "colab_type": "text"
      },
      "source": [
        "## Predict, compare, and learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TcELK-6axYM",
        "colab_type": "text"
      },
      "source": [
        "We learned about the paradigm “predict, compare, learn,” and we dove\n",
        "deep into the first step: **predict**. So now we cover the next two steps of the paradigm: **compare and learn**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRNR9rlmbMYC",
        "colab_type": "text"
      },
      "source": [
        "### Compare\n",
        "\n",
        "**Comparing gives a measurement of how much a prediction\n",
        "“missed” by.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xJekwbfbNd8",
        "colab_type": "text"
      },
      "source": [
        "Once you’ve made a prediction, the next step is to evaluate how well you did. This may\n",
        "seem like a simple concept, but you’ll find that coming up with a good way to measure\n",
        "error is one of the most important and complicated subjects of deep learning.\n",
        "\n",
        "You’ll also learn that error is always positive! We’ll consider the analogy of an\n",
        "archer hitting a target: whether the shot is too low by an inch or too high by an inch, the\n",
        "error is still just 1 inch. \n",
        "\n",
        "In the neural network compare step, you need to consider these\n",
        "kinds of properties when measuring error.\n",
        "\n",
        "we evaluate only one simple way of measuring error: mean\n",
        "squared error. It’s but one of many ways to evaluate the accuracy of a neural network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caHQp839cazu",
        "colab_type": "text"
      },
      "source": [
        "### Learn\n",
        "**Learning tells each weight how it can change to reduce the error.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM5b6IYVcgYv",
        "colab_type": "text"
      },
      "source": [
        "Learning is all about error attribution, or the art of figuring out how each weight played its part in creating error. It’s the blame game of deep learning.\n",
        "\n",
        "So we’ll spend times looking at the most popular version of the deep learning blame game:**gradient descent**.\n",
        "\n",
        "At the end of the day, it results in computing a number for each weight. That number\n",
        "represents how that weight should be higher or lower in order to reduce the error. Then\n",
        "you’ll move the weight according to that number, and you’ll be finished."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9ZNdXuEdfWQ",
        "colab_type": "text"
      },
      "source": [
        "## Compare: Does your network make good predictions?\n",
        "**Let’s measure the error and find out!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBiPsJJid01W",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/measure-error-1.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp4zZfBakuLx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2318f8ee-f0aa-426e-c7b1-b782d94c76bd"
      },
      "source": [
        "knob_weight = 0.5\n",
        "input = 0.5\n",
        "goal_pred = 0.8\n",
        "\n",
        "pred = input * knob_weight\n",
        "\n",
        "error = (pred - goal_pred) ** 2\n",
        "print(error)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.30250000000000005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hzZyZULpDgK",
        "colab_type": "text"
      },
      "source": [
        "## Why measure error?\n",
        "**Measuring error simplifies the problem.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBPiJ5ZwpHk9",
        "colab_type": "text"
      },
      "source": [
        "The goal of training a neural network is to make correct predictions. That’s what you want.\n",
        "And in the most pragmatic world you want the\n",
        "network to take input that you can easily calculate (today’s stock price) and predict things that\n",
        "are hard to calculate (tomorrow’s stock price). That’s what makes a neural network useful.\n",
        "\n",
        "It turns out that changing knob_weight to make the network correctly predict\n",
        "goal_prediction is slightly more complicated than changing knob_weight to make\n",
        "error == 0. There’s something more concise about looking at the problem this way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAk1f-AKlG_1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "90cd2a23-ecc9-469c-fb82-fe4b2ebe1cee"
      },
      "source": [
        "knob_weight = 0.9\n",
        "input = 0.5\n",
        "goal_pred = 0.8\n",
        "\n",
        "pred = input * knob_weight\n",
        "\n",
        "error = (pred - goal_pred) ** 2\n",
        "print(error)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.12250000000000003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPzYLlJbroN7",
        "colab_type": "text"
      },
      "source": [
        "**Different ways of measuring error prioritize error differently.**\n",
        "\n",
        "By squaring the error, numbers that are less than 1 get smaller, whereas numbers that are greater than 1 get bigger. You’re going to change what I call pure error (pred - goal_pred) so that bigger errors become very big and smaller errors quickly become irrelevant.\n",
        "\n",
        "By measuring error this way, you can prioritize big errors over smaller ones. When you have\n",
        "somewhat large pure errors (say, 10), you’ll tell yourself that you have very large error $$(10**2 == 100)$$ and in contrast, when you have small pure errors (say, 0.01), you’ll tell yourself that you\n",
        "have very small error $$(0.01**2 == 0.0001)$$ See what I mean about prioritizing? It’s just modifying\n",
        "what you consider to be error so that you amplify big ones and largely ignore small ones.\n",
        "\n",
        "In contrast, if you took the absolute value instead of squaring the error, you wouldn’t have this\n",
        "type of prioritization. The error would just be the positive version of the pure error—which\n",
        "would be fine, but different.\n",
        "\n",
        "**Why do you want only positive error?**\n",
        "\n",
        "Eventually, you’ll be working with millions of input -> goal_prediction pairs, and we’ll\n",
        "still want to make accurate predictions. So, you’ll try to take the average error down to 0.\n",
        "\n",
        "This presents a problem if the error can be positive and negative.\n",
        "\n",
        "Imagine if you were\n",
        "trying to get the neural network to correctly predict two datapoints—two input ->\n",
        "goal_prediction pairs. \n",
        "\n",
        "If the first had an error of 1,000 and the second had an error of\n",
        "–1,000, then the average error would be zero! \n",
        "\n",
        "You’d fool yourself into thinking you predicted\n",
        "perfectly, when you missed by 1,000 each time! That would be really bad. \n",
        "\n",
        "Thus, you want the\n",
        "error of each prediction to always be positive so they don’t accidentally cancel each other out\n",
        "when you average them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3BvEp4UrUic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "2"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}