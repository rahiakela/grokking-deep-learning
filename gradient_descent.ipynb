{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gradient-descent.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/grokking-deep-learning/blob/4-gradient-descent/gradient_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUDd4oT1agdf",
        "colab_type": "text"
      },
      "source": [
        "# introduction to neural learning: gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZB9oWbPEbGC_",
        "colab_type": "text"
      },
      "source": [
        "## Predict, compare, and learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TcELK-6axYM",
        "colab_type": "text"
      },
      "source": [
        "We learned about the paradigm “predict, compare, learn,” and we dove\n",
        "deep into the first step: **predict**. So now we cover the next two steps of the paradigm: **compare and learn**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRNR9rlmbMYC",
        "colab_type": "text"
      },
      "source": [
        "### Compare\n",
        "\n",
        "**Comparing gives a measurement of how much a prediction\n",
        "“missed” by.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xJekwbfbNd8",
        "colab_type": "text"
      },
      "source": [
        "Once you’ve made a prediction, the next step is to evaluate how well you did. This may\n",
        "seem like a simple concept, but you’ll find that coming up with a good way to measure\n",
        "error is one of the most important and complicated subjects of deep learning.\n",
        "\n",
        "You’ll also learn that error is always positive! We’ll consider the analogy of an\n",
        "archer hitting a target: whether the shot is too low by an inch or too high by an inch, the\n",
        "error is still just 1 inch. \n",
        "\n",
        "In the neural network compare step, you need to consider these\n",
        "kinds of properties when measuring error.\n",
        "\n",
        "we evaluate only one simple way of measuring error: mean\n",
        "squared error. It’s but one of many ways to evaluate the accuracy of a neural network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caHQp839cazu",
        "colab_type": "text"
      },
      "source": [
        "### Learn\n",
        "**Learning tells each weight how it can change to reduce the error.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM5b6IYVcgYv",
        "colab_type": "text"
      },
      "source": [
        "Learning is all about error attribution, or the art of figuring out how each weight played its part in creating error. It’s the blame game of deep learning.\n",
        "\n",
        "So we’ll spend times looking at the most popular version of the deep learning blame game:**gradient descent**.\n",
        "\n",
        "At the end of the day, it results in computing a number for each weight. That number\n",
        "represents how that weight should be higher or lower in order to reduce the error. Then\n",
        "you’ll move the weight according to that number, and you’ll be finished."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9ZNdXuEdfWQ",
        "colab_type": "text"
      },
      "source": [
        "## Compare: Does your network make good predictions?\n",
        "**Let’s measure the error and find out!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBiPsJJid01W",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/measure-error-1.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp4zZfBakuLx",
        "colab_type": "code",
        "outputId": "2318f8ee-f0aa-426e-c7b1-b782d94c76bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "knob_weight = 0.5\n",
        "input = 0.5\n",
        "goal_pred = 0.8\n",
        "\n",
        "pred = input * knob_weight\n",
        "\n",
        "error = (pred - goal_pred) ** 2\n",
        "print(error)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.30250000000000005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hzZyZULpDgK",
        "colab_type": "text"
      },
      "source": [
        "## Why measure error?\n",
        "**Measuring error simplifies the problem.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBPiJ5ZwpHk9",
        "colab_type": "text"
      },
      "source": [
        "The goal of training a neural network is to make correct predictions. That’s what you want.\n",
        "And in the most pragmatic world you want the\n",
        "network to take input that you can easily calculate (today’s stock price) and predict things that\n",
        "are hard to calculate (tomorrow’s stock price). That’s what makes a neural network useful.\n",
        "\n",
        "It turns out that changing knob_weight to make the network correctly predict\n",
        "goal_prediction is slightly more complicated than changing knob_weight to make\n",
        "error == 0. There’s something more concise about looking at the problem this way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAk1f-AKlG_1",
        "colab_type": "code",
        "outputId": "90cd2a23-ecc9-469c-fb82-fe4b2ebe1cee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "knob_weight = 0.9\n",
        "input = 0.5\n",
        "goal_pred = 0.8\n",
        "\n",
        "pred = input * knob_weight\n",
        "\n",
        "error = (pred - goal_pred) ** 2\n",
        "print(error)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.12250000000000003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPzYLlJbroN7",
        "colab_type": "text"
      },
      "source": [
        "**Different ways of measuring error prioritize error differently.**\n",
        "\n",
        "By squaring the error, numbers that are less than 1 get smaller, whereas numbers that are greater than 1 get bigger. You’re going to change what I call pure error (pred - goal_pred) so that bigger errors become very big and smaller errors quickly become irrelevant.\n",
        "\n",
        "By measuring error this way, you can prioritize big errors over smaller ones. When you have\n",
        "somewhat large pure errors (say, 10), you’ll tell yourself that you have very large error $$(10**2 == 100)$$ and in contrast, when you have small pure errors (say, 0.01), you’ll tell yourself that you\n",
        "have very small error $$(0.01**2 == 0.0001)$$ See what I mean about prioritizing? It’s just modifying\n",
        "what you consider to be error so that you amplify big ones and largely ignore small ones.\n",
        "\n",
        "In contrast, if you took the absolute value instead of squaring the error, you wouldn’t have this\n",
        "type of prioritization. The error would just be the positive version of the pure error—which\n",
        "would be fine, but different.\n",
        "\n",
        "**Why do you want only positive error?**\n",
        "\n",
        "Eventually, you’ll be working with millions of input -> goal_prediction pairs, and we’ll\n",
        "still want to make accurate predictions. So, you’ll try to take the average error down to 0.\n",
        "\n",
        "This presents a problem if the error can be positive and negative.\n",
        "\n",
        "Imagine if you were\n",
        "trying to get the neural network to correctly predict two datapoints—two input ->\n",
        "goal_prediction pairs. \n",
        "\n",
        "If the first had an error of 1,000 and the second had an error of\n",
        "–1,000, then the average error would be zero! \n",
        "\n",
        "You’d fool yourself into thinking you predicted\n",
        "perfectly, when you missed by 1,000 each time! That would be really bad. \n",
        "\n",
        "Thus, you want the\n",
        "error of each prediction to always be positive so they don’t accidentally cancel each other out\n",
        "when you average them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbEwXrLhuF4T",
        "colab_type": "text"
      },
      "source": [
        "## What’s the simplest form of neural learning?\n",
        "\n",
        "**Learning using the hot and cold method.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcLpn5QjuOTu",
        "colab_type": "text"
      },
      "source": [
        "At the end of the day, learning is really about one thing: adjusting knob_weight either up\n",
        "or down so the error is reduced. If you keep doing this and the error goes to 0, you’re done\n",
        "learning! How do you know whether to turn the knob up or down? Well, you try both up and\n",
        "down and see which one reduces the error! Whichever one reduces the error is used to update\n",
        "knob_weight. It’s simple but effective. After you do this over and over again, eventually\n",
        "error == 0, which means the neural network is predicting with perfect accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mlDgKaiubhk",
        "colab_type": "text"
      },
      "source": [
        "### Hot and cold learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDMM64KOucnK",
        "colab_type": "text"
      },
      "source": [
        "Hot and cold learning means wiggling the weights to see which direction reduces the error\n",
        "the most, moving the weights in that direction, and repeating until the error gets to 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LRlku00xHeA",
        "colab_type": "text"
      },
      "source": [
        "### An empty network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twDuGG0swbFT",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hot-and-cold-learning-1.JPG?raw=1' width='800'/>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3BvEp4UrUic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight = 0.1\n",
        "lr = 0.01\n",
        "\n",
        "def neural_network(input, weight):\n",
        "  prediction = input * weight\n",
        "  return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmDB-dA7xLyn",
        "colab_type": "text"
      },
      "source": [
        "### PREDICT: Making a prediction and evaluating error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mo9OfQnxQrH",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hot-and-cold-learning-2.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQg5DMCCw_2v",
        "colab_type": "code",
        "outputId": "9a3ccf7c-23f1-41c2-b074-8d8ce18465c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "number_of_toes = [8.5]\n",
        "win_or_lose_binary = [1] # (won!!!)\n",
        "\n",
        "input = number_of_toes[0]\n",
        "true = win_or_lose_binary[0]\n",
        "\n",
        "pred = neural_network(input, weight)\n",
        "\n",
        "error = (pred - true) ** 2\n",
        "print(error)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.022499999999999975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee3remxlx7lx",
        "colab_type": "text"
      },
      "source": [
        "### COMPARE: Making a prediction with a higher weight and evaluating error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6X_MhDCx8to",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hot-and-cold-learning-3.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3Wz18CVxxYl",
        "colab_type": "code",
        "outputId": "eeeaf9e2-788e-4da9-bfcb-ca18c1242572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "lr = 0.1 # higher\n",
        "\n",
        "pred_up = neural_network(input, weight + lr)\n",
        "\n",
        "err_up = (pred_up - true) ** 2\n",
        "print(err_up)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.49000000000000027\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEsEMaH2yZeu",
        "colab_type": "code",
        "outputId": "b3e99946-2791-41ae-c1ad-1c626da2a912",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "lr = 0.01  # lower\n",
        "\n",
        "pred_down = neural_network(input, weight - lr)\n",
        "\n",
        "err_down = (pred_down - true) ** 2\n",
        "print(err_down)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.05522499999999994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5-vV9BuzJXM",
        "colab_type": "text"
      },
      "source": [
        "### COMPARE + LEARN: Comparing the errors and setting the new weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fADcUTRgzKvI",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hot-and-cold-learning-4.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZABEiCOy3tY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if (error > err_down or error > err_up):\n",
        "  if ( err_down < err_up):\n",
        "    weight -= lr\n",
        "  if (err_up < err_up):\n",
        "    weight += lr  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9DteFxa0SsN",
        "colab_type": "text"
      },
      "source": [
        "This reveals what learning in neural networks really is: a search problem. You’re searching\n",
        "for the best possible configuration of weights so the network’s error falls to 0 (and predicts\n",
        "perfectly). As with all other forms of search, you might not find exactly what you’re looking\n",
        "for, and even if you do, it may take some time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fanpAjs0V-F",
        "colab_type": "text"
      },
      "source": [
        "## Hot and cold learning\n",
        "**This is perhaps the simplest form of learning.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k24XTFJN3MvR",
        "colab_type": "text"
      },
      "source": [
        "#### Complete Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTqXlYC6zspO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight = 0.5\n",
        "input = 0.5\n",
        "\n",
        "goal_prediction = 0.8\n",
        "\n",
        "step_amount = 0.001  # How much to move the weights each iteration\n",
        "\n",
        "# Repeat learning many times so the error can keep getting smaller.\n",
        "for iteration in range(1101):\n",
        "  prediction = input * weight\n",
        "  error = (prediction - goal_prediction) ** 2\n",
        "\n",
        "  print(f'Error: {str(error)}\\t\\tPrediction: {str(prediction)}')\n",
        "\n",
        "  up_prediction = input * (weight + step_amount)   # try up!\n",
        "  up_error = (goal_prediction - up_prediction) ** 2\n",
        "\n",
        "  down_prediction = input * (weight - step_amount)  # try down!\n",
        "  down_error = (goal_prediction - down_prediction) ** 2\n",
        "\n",
        "  if (down_error < up_error):\n",
        "    weight = weight - step_amount    # If down is better, go down!\n",
        "  if (down_error > up_error):\n",
        "    weight = weight + step_amount    # If up is better, go up!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4QV-UGs3GDi",
        "colab_type": "text"
      },
      "source": [
        "### Characteristics of hot and cold learning\n",
        "**It’s simple.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f7tmYik4Fi2",
        "colab_type": "text"
      },
      "source": [
        "Hot and cold learning is simple. After making a prediction, you predict two more times, once with a\n",
        "slightly higher weight and again with a slightly lower weight. You then move weight depending on\n",
        "which direction gave a smaller error. Repeating this enough times eventually reduces error to 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqMA2Bo3_0OT",
        "colab_type": "text"
      },
      "source": [
        "####Problem 1: It’s inefficient.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4ReA-jJ_8pL",
        "colab_type": "text"
      },
      "source": [
        "You have to predict multiple times to make a single knob_weight update. This seems very inefficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Spz5XK-LAAqA",
        "colab_type": "text"
      },
      "source": [
        "#### Problem 2: Sometimes it’s impossible to predict the exact goal prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U58BXsGHAFJC",
        "colab_type": "text"
      },
      "source": [
        "With a set step_amount, unless the perfect weight is exactly n*step_amount away, the network\n",
        "will eventually overshoot by some number less than step_amount. When it does, it will then\n",
        "start alternating back and forth between each side of goal_prediction. Set step_amount to 0.2\n",
        "to see this in action. If you set step_amount to 10, you’ll really break it. When I try this, I see the\n",
        "following output.\n",
        "\n",
        "It never remotely comes close to 0.8!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqEx5rq72heo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight = 0.5\n",
        "input = 0.5\n",
        "\n",
        "goal_prediction = 0.8\n",
        "\n",
        "step_amount = 0.2  # How much to move the weights each iteration\n",
        "\n",
        "# Repeat learning many times so the error can keep getting smaller.\n",
        "for iteration in range(1101):\n",
        "  prediction = input * weight\n",
        "  error = (prediction - goal_prediction) ** 2\n",
        "\n",
        "  print(f'Error: {str(error)}\\t\\tPrediction: {str(prediction)}')\n",
        "\n",
        "  up_prediction = input * (weight + step_amount)   # try up!\n",
        "  up_error = (goal_prediction - up_prediction) ** 2\n",
        "\n",
        "  down_prediction = input * (weight - step_amount)  # try down!\n",
        "  down_error = (goal_prediction - down_prediction) ** 2\n",
        "\n",
        "  if (down_error < up_error):\n",
        "    weight = weight - step_amount    # If down is better, go down!\n",
        "  if (down_error > up_error):\n",
        "    weight = weight + step_amount    # If up is better, go up!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK0e2sORAwSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight = 0.5\n",
        "input = 0.5\n",
        "\n",
        "goal_prediction = 0.8\n",
        "\n",
        "step_amount = 10  # How much to move the weights each iteration\n",
        "\n",
        "# Repeat learning many times so the error can keep getting smaller.\n",
        "for iteration in range(1101):\n",
        "  prediction = input * weight\n",
        "  error = (prediction - goal_prediction) ** 2\n",
        "\n",
        "  print(f'Error: {str(error)}\\t\\tPrediction: {str(prediction)}')\n",
        "\n",
        "  up_prediction = input * (weight + step_amount)   # try up!\n",
        "  up_error = (goal_prediction - up_prediction) ** 2\n",
        "\n",
        "  down_prediction = input * (weight - step_amount)  # try down!\n",
        "  down_error = (goal_prediction - down_prediction) ** 2\n",
        "\n",
        "  if (down_error < up_error):\n",
        "    weight = weight - step_amount    # If down is better, go down!\n",
        "  if (down_error > up_error):\n",
        "    weight = weight + step_amount    # If up is better, go up!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtTWsGALBCE7",
        "colab_type": "text"
      },
      "source": [
        "The real problem is that even though you know the correct direction to move weight, you don’t know\n",
        "the correct amount. Instead, you pick a fixed one at random (step_amount). Furthermore, this amount\n",
        "has nothing to do with error. Whether error is big or tiny, step_amount is the same. \n",
        "\n",
        "So, hot and cold\n",
        "learning is kind of a bummer. It’s inefficient because you predict three times for each weight update, and\n",
        "step_ amount is arbitrary, which can prevent you from learning the correct weight value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE5lEN17BYQp",
        "colab_type": "text"
      },
      "source": [
        "## Calculating both direction and amount from error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_Cs4gdi4ksu",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/direction-and-amount.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOBsvbKSA2pA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight = 0.5\n",
        "goal_pred = 0.8\n",
        "input = 0.5\n",
        "\n",
        "for iteration in range(20):\n",
        "  pred = input * weight\n",
        "  error = (pred - goal_pred) ** 2\n",
        "  direction_and_amount = (pred - goal_pred) * input\n",
        "  weight = weight - direction_and_amount\n",
        "  print(f'Error: {str(error)} \\t\\t\\t Prediction: {str(pred)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDUjafU16FaR",
        "colab_type": "text"
      },
      "source": [
        "## One iteration of gradient descent\n",
        "\n",
        "**This performs a weight update on a single training example\n",
        "(input->true) pair.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK14-kTv6HQU",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/gradient-descent-1.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFXKwLId5l8S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight = 0.1\n",
        "alpha = 0.01\n",
        "\n",
        "def neural_network(input , weight):\n",
        "  prediction = input * weight\n",
        "  return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebewGj6L6lPB",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/gradient-descent-2.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxWs4TZF6i3g",
        "colab_type": "code",
        "outputId": "e7388ce4-1a92-434d-89c1-11050af79732",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "number_of_toes = [8.5]\n",
        "win_or_lose_binary = [1]  # (won!!!)\n",
        "\n",
        "input = number_of_toes[0]\n",
        "goal_pred = win_or_lose_binary[0]\n",
        "\n",
        "pred = neural_network(input, weight)\n",
        "\n",
        "error = (pred - goal_pred) ** 2\n",
        "print(error)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.022499999999999975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKoX3J_t7om2",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/gradient-descent-3.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "behmF1Si7m6t",
        "colab_type": "code",
        "outputId": "a4dfed0c-17ef-4f7a-9feb-75901deca5fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "number_of_toes = [8.5]\n",
        "win_or_lose_binary = [1]  # (won!!!)\n",
        "\n",
        "input = number_of_toes[0]\n",
        "goal_pred = win_or_lose_binary[0]\n",
        "\n",
        "pred = neural_network(input, weight)\n",
        "print(pred)\n",
        "\n",
        "delta = pred - goal_pred\n",
        "print(delta)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8500000000000001\n",
            "-0.1499999999999999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpxnymUb8wr5",
        "colab_type": "text"
      },
      "source": [
        "delta is a measurement of how much this node missed. The true prediction is 1.0, and the network’s prediction was 0.85, so the network was too low by 0.15. Thus, delta is negative 0.15.\n",
        "\n",
        "The primary difference between gradient descent and this implementation is the new variable\n",
        "delta. It’s the raw amount that the node was too high or too low. Instead of computing\n",
        "direction_and_amount directly, you first calculate how much you want the output node to be\n",
        "different. Only then do you compute direction_and_amount to change weight (in step 4, now\n",
        "renamed weight_delta):\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/gradient-descent-4.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAvZ06XT8PsH",
        "colab_type": "code",
        "outputId": "13641527-f12d-4dfd-c3a9-0f062bcd9bd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "number_of_toes = [8.5]\n",
        "win_or_lose_binary = [1]  # (won!!!)\n",
        "\n",
        "input = number_of_toes[0]\n",
        "goal_pred = win_or_lose_binary[0]\n",
        "\n",
        "pred = neural_network(input, weight)\n",
        "print(pred)\n",
        "\n",
        "error = (pred - goal_pred) ** 2\n",
        "print(error)\n",
        "\n",
        "delta = pred - goal_pred\n",
        "print(delta)\n",
        "\n",
        "weight_delta = input * delta\n",
        "print(weight_delta)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8500000000000001\n",
            "0.022499999999999975\n",
            "-0.1499999999999999\n",
            "-1.2749999999999992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du_MJ2E-99WP",
        "colab_type": "text"
      },
      "source": [
        "weight_delta is a measure of how much a weight caused the network to miss. You calculate\n",
        "it by multiplying the weight’s output node delta by the weight’s input. Thus, you create\n",
        "each weight_delta by scaling its output node delta by the weight’s input. This accounts\n",
        "for the three aforementioned properties of direction_and_amount: scaling, negative\n",
        "reversal, and stopping.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/gradient-descent-6.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GhLKG9S-LkZ",
        "colab_type": "code",
        "outputId": "47fb1979-256d-4683-e587-63745fe3205a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "number_of_toes = [8.5]\n",
        "win_or_lose_binary = [1]  # (won!!!)\n",
        "\n",
        "input = number_of_toes[0]\n",
        "goal_pred = win_or_lose_binary[0]\n",
        "\n",
        "pred = neural_network(input, weight)\n",
        "print(pred)\n",
        "\n",
        "error = (pred - goal_pred) ** 2\n",
        "print(error)\n",
        "\n",
        "delta = pred - goal_pred\n",
        "print(delta)\n",
        "\n",
        "weight_delta = input * delta\n",
        "print(weight_delta)\n",
        "\n",
        "alpha = 0.01  # learning rate\n",
        "weight -= weight_delta * alpha\n",
        "print(weight)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8500000000000001\n",
            "0.022499999999999975\n",
            "-0.1499999999999999\n",
            "-1.2749999999999992\n",
            "0.11275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg2ytqDT_AW7",
        "colab_type": "text"
      },
      "source": [
        "You multiply weight_delta by a small number alpha before using it to update weight. This lets you control how fast the network learns. If it learns too fast, it can update weights too aggressively and overshoot.\n",
        "\n",
        "Note that the weight update made the same change (small increase) as hot and cold learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf_ZfCby_S6e",
        "colab_type": "text"
      },
      "source": [
        "## Learning is just reducing error\n",
        "**You can modify weight to reduce error.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIar_Q_D-tn9",
        "colab_type": "code",
        "outputId": "b661bbc9-c9e0-496a-c933-9218c3e46add",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "weight, goal_pred, input = (0.0, 0.8, 0.5)\n",
        "\n",
        "for i in range(4):\n",
        "  # These lines have a secret.\n",
        "  pred = input * weight             # multiply weight parameter and input feature\n",
        "  error = (pred - goal_pred) ** 2   # measure the diffrerence of predicted value and target value\n",
        "\n",
        "  delta = pred - goal_pred          # get the diffrerence(missing value) of predicted value and target value\n",
        "  weight_delta = input * delta      # multiply again input feature by the diffrerence(missing value)\n",
        "  weight = weight - weight_delta    # adjust the weight by subtracting with multiplied input feature and the diffrerence(missing value)\n",
        "  print(f'Error: {str(error)} \\t\\t Prediction: {str(pred)}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error: 0.6400000000000001 \t\t Prediction: 0.0\n",
            "Error: 0.3600000000000001 \t\t Prediction: 0.2\n",
            "Error: 0.2025 \t\t Prediction: 0.35000000000000003\n",
            "Error: 0.11390625000000001 \t\t Prediction: 0.4625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LVLn3WN5A9n",
        "colab_type": "text"
      },
      "source": [
        "**This approach adjusts each weight in the correct direction and by the correct amount so that error reduces to 0.**\n",
        "\n",
        "All you’re trying to do is figure out the right direction and amount to modify weight so that\n",
        "error goes down. The secret lies in the pred and error calculations. Notice that you use pred\n",
        "inside the error calculation.\n",
        "\n",
        "Let’s replace the pred variable with the code used to generate it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsbOQBE82d_W",
        "colab_type": "code",
        "outputId": "73a171cd-5597-49d8-ec99-7543e4d46ecc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "error = ((input * weight) - goal_pred) ** 2\n",
        "print(error)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.06407226562500003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtPW8CZl5m6a",
        "colab_type": "text"
      },
      "source": [
        "This doesn’t change the value of error at all! It just combines the two lines of code and computes error directly.\n",
        "\n",
        "Remember that input and goal_prediction are fixed at 0.5 and\n",
        "0.8, respectively (you set them before the network starts training). \n",
        "\n",
        "So, if you replace their\n",
        "variables names with the values, the secret becomes clear:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6HpuKl96DDO",
        "colab_type": "code",
        "outputId": "e5e9bcbc-f2b8-400a-ed51-a7b6ffff6f92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "error = ((0.5 * weight) - 0.8) ** 2\n",
        "print(error)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.06407226562500003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Mtf43Ks6OIQ",
        "colab_type": "text"
      },
      "source": [
        "Let’s say you increased weight by 0.5. If there’s an exact relationship between error and weight,\n",
        "you should be able to calculate how much this also moves error. What if you wanted to move\n",
        "error in a specific direction? Could it be done?\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/reduce-error-1.JPG?raw=1' width='800'/>\n",
        "\n",
        "This graph represents every value of error for every weight according to the relationship in the\n",
        "previous formula. Notice it makes a nice bowl shape. The black dot is at the point of both the\n",
        "current weight and error. The dotted circle is where you want to be (error == 0).\n",
        "\n",
        "**The slope points to the bottom of the bowl (lowest error) no matter where you are in the\n",
        "bowl. You can use this slope to help the neural network reduce the error.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdxuQALi7EgX",
        "colab_type": "text"
      },
      "source": [
        "## Let’s watch several steps of learning\n",
        "\n",
        "**Will we eventually find the bottom of the bowl?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ErgKcAj7924",
        "colab_type": "code",
        "outputId": "cb6eaa82-9d62-49ad-8cdb-7b641f1b31ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "weight, goal_pred, input = (0.0, 0.8, 1.1)\n",
        "\n",
        "for i in range(4):\n",
        "\n",
        "  print(f'-----------\\nWeight: {str(weight)}')\n",
        "\n",
        "  # These lines have a secret.\n",
        "  pred = input * weight             # multiply weight parameter and input feature\n",
        "  error = (pred - goal_pred) ** 2   # measure the diffrerence of predicted value and target value\n",
        "\n",
        "  delta = pred - goal_pred          # get the diffrerence(missing value) of predicted value and target value\n",
        "  weight_delta = input * delta      # multiply again input feature by the diffrerence(missing value)\n",
        "  weight = weight - weight_delta    # adjust the weight by subtracting with multiplied input feature and the diffrerence(missing value)\n",
        "  print(f'Error: {str(error)}  Prediction: {str(pred)}')\n",
        "  print(f'Delta: {str(delta)}  Weight Delta: {str(weight_delta)}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------\n",
            "Weight: 0.0\n",
            "Error: 0.6400000000000001  Prediction: 0.0\n",
            "Delta: -0.8  Weight Delta: -0.8800000000000001\n",
            "-----------\n",
            "Weight: 0.8800000000000001\n",
            "Error: 0.02822400000000005  Prediction: 0.9680000000000002\n",
            "Delta: 0.16800000000000015  Weight Delta: 0.1848000000000002\n",
            "-----------\n",
            "Weight: 0.6951999999999999\n",
            "Error: 0.0012446784000000064  Prediction: 0.76472\n",
            "Delta: -0.03528000000000009  Weight Delta: -0.0388080000000001\n",
            "-----------\n",
            "Weight: 0.734008\n",
            "Error: 5.4890317439999896e-05  Prediction: 0.8074088\n",
            "Delta: 0.007408799999999993  Weight Delta: 0.008149679999999992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDKW4oOu8cGq",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/reduce-error-2.JPG?raw=1' width='800'/>\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/reduce-error-3.JPG?raw=1' width='800'/>\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/reduce-error-4.JPG?raw=1' width='800'/>\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/reduce-error-5.JPG?raw=1' width='800'/>\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/reduce-error-6.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-x9MrYvAjW-",
        "colab_type": "text"
      },
      "source": [
        "## Why does this work? What is weight_delta, really?\n",
        "\n",
        "**Let’s back up and talk about functions. What is a function?\n",
        "How do you understand one?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_Uq8LfkCEKR",
        "colab_type": "text"
      },
      "source": [
        "Consider this function:\n",
        "\n",
        "def my_function(x):\n",
        "   return x * 2\n",
        "\n",
        "Every function has what you might call moving parts: pieces you can tweak or change to make\n",
        "the output the function generates different. Consider my_function in the previous example. Ask\n",
        "yourself, “What’s controlling the relationship between the input and the output of this function?”\n",
        "The answer is, the 2. Ask the same question about the following function:\n",
        "\n",
        "$$error = ((input * weight) - goal_pred) ** 2$$\n",
        "\n",
        "What’s controlling the relationship between input and the output (error)? Plenty of things\n",
        "are—this function is a bit more complicated! goal_pred, input, **2, weight, and all the\n",
        "parentheses and algebraic operations (addition, subtraction, and so on) play a part in calculating\n",
        "the error. Tweaking any one of them would change the error. This is important to consider.\n",
        "\n",
        "As a thought exercise, consider changing goal_pred to reduce the error. This is silly, but totally\n",
        "doable. In life, you might call this (setting goals to be whatever your capability is) “giving up.”\n",
        "You’re denying that you missed! That wouldn’t do.\n",
        "\n",
        "What if you changed input until error went to 0? Well, that’s akin to seeing the world as you\n",
        "want to see it instead of as it actually is. You’re changing the input data until you’re predicting\n",
        "what you want to predict (this is loosely how inceptionism works).\n",
        "\n",
        "Now consider changing the 2, or the additions, subtractions, or multiplications. This is just\n",
        "changing how you calculate error in the first place. The error calculation is meaningless if\n",
        "it doesn’t actually give a good measure of how much you missed (with the right properties\n",
        "mentioned a few pages ago). This won’t do, either.\n",
        "\n",
        "What’s left? The only variable remaining is weight. Adjusting it doesn’t change your perception\n",
        "of the world, doesn’t change your goal, and doesn’t destroy your error measure. Changing\n",
        "weight means the function conforms to the patterns in the data. By forcing the rest of the\n",
        "function to be unchanging, you force the function to correctly model some pattern in the data.\n",
        "It’s only allowed to modify how the network predicts.\n",
        "\n",
        "To sum up: you modify specific parts of an error function until the error value goes to 0. This error\n",
        "function is calculated using a combination of variables, some of which you can change (weights) and\n",
        "some of which you can’t (input data, output data, and the error logic):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDaZ2EKL8Z_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight = 0.5\n",
        "goal_pred = 0.8\n",
        "input = 0.5\n",
        "\n",
        "for i in range(20):\n",
        "  print(f'-----------\\nWeight: {str(weight)}')\n",
        "\n",
        "  # These lines have a secret.\n",
        "  pred = input * weight             # multiply weight parameter and input feature\n",
        "  error = (pred - goal_pred) ** 2   # measure the diffrerence of predicted value and target value\n",
        "\n",
        "  # get the diffrerence(missing value) of predicted value and target value\n",
        "  # multiply again input feature by the diffrerence(missing value)\n",
        "  direction_and_amount = (pred - goal_pred) * input    \n",
        "  # adjust the weight by subtracting with multiplied input feature and the diffrerence(missing value)  \n",
        "  weight = weight - direction_and_amount   \n",
        "  print(f'Error: {str(error)}  Prediction: {str(pred)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKDH5XR1EJSY",
        "colab_type": "text"
      },
      "source": [
        "Learning is all about automatically changing the prediction function so that it\n",
        "makes good predictions—aka, so that the subsequent error goes down to 0.\n",
        "\n",
        "Now that you know what you’re allowed to change, how do you go about doing the changing?\n",
        "That’s the good stuff. That’s the machine learning, right? In the next section, we’re going to talk\n",
        "about exactly that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNFJJX0SEfHE",
        "colab_type": "text"
      },
      "source": [
        "##How to use a derivative to learn\n",
        "**weight_delta is your derivative.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXt5oiQgC1Nv",
        "colab_type": "text"
      },
      "source": [
        "Error is a measure of how much you missed. The\n",
        "derivative defines the relationship between each\n",
        "weight and how much you missed. In other\n",
        "words, it tells how much changing a weight\n",
        "contributed to the error. So, now that you know\n",
        "this, how do you use it to move the error in a\n",
        "particular direction?\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/derivative-1.JPG?raw=1' width='800'/>\n",
        "\n",
        "You’ve learned the relationship between\n",
        "two variables in a function, but how do you\n",
        "exploit that relationship? As it turns out, this\n",
        "is incredibly visual and intuitive. Check out\n",
        "the error curve again. The black dot is where\n",
        "weight starts out: (0.5). The dotted circle is where you want it to go: the goal weight. Do you see\n",
        "the dotted line attached to the black dot? That’s the slope, otherwise known as the derivative. It\n",
        "tells you at that point in the curve how much error changes when you change weight. Notice\n",
        "that it’s pointed downward: it’s a negative slope.\n",
        "\n",
        "The slope of a line or curve always points in the opposite direction of the lowest point of the line or\n",
        "curve. So, if you have a negative slope, you increase weight to find the minimum of error.\n",
        "\n",
        "Remember back to the goal again: you’re trying to figure out the direction and the amount to\n",
        "change the weight so the error goes down. A derivative gives you the relationship between any\n",
        "two variables in a function. You use the derivative to determine the relationship between any\n",
        "weight and error. You then move the weight in the opposite direction of the derivative to find the\n",
        "lowest weight. Voilà! The neural network learns.\n",
        "\n",
        "**This method for learning (finding error minimums) is called gradient descent.** This name should\n",
        "seem intuitive. You move the weight value opposite the gradient value, which reduces error to 0. By opposite, I mean you increase the weight when you have a negative gradient, and vice versa.\n",
        "It’s like gravity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTb1lD5hEAua",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "f515852c-99de-4503-80ac-e949ada1d01b"
      },
      "source": [
        "weight = 0.0\n",
        "goal_pred = 0.8\n",
        "input = 1.1\n",
        "\n",
        "for i in range(4):\n",
        "  pred = input * weight\n",
        "  error = (pred - goal_pred) ** 2\n",
        "  delta = pred - goal_pred\n",
        "  weight_delta = delta * input   # Derivative(how fast the error changes, given changes in the weight)\n",
        "  weight = weight - weight_delta\n",
        "  print(f'Error: {str(error)} Prediction: {str(pred)} Weight: {str(weight)} Weight Delta: {str(weight_delta)} Delta: {str(delta)}')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error: 0.6400000000000001 Prediction: 0.0 Weight: 0.8800000000000001 Weight Delta: -0.8800000000000001 Delta: -0.8\n",
            "Error: 0.02822400000000005 Prediction: 0.9680000000000002 Weight: 0.6951999999999999 Weight Delta: 0.1848000000000002 Delta: 0.16800000000000015\n",
            "Error: 0.0012446784000000064 Prediction: 0.76472 Weight: 0.734008 Weight Delta: -0.0388080000000001 Delta: -0.03528000000000009\n",
            "Error: 5.4890317439999896e-05 Prediction: 0.8074088 Weight: 0.72585832 Weight Delta: 0.008149679999999992 Delta: 0.007408799999999993\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLZW-NBsHvfR",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/derivative-2.JPG?raw=1' width='800'/>\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/derivative-3.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSTR83B-IVS0",
        "colab_type": "text"
      },
      "source": [
        "## Breaking gradient descent\n",
        "**Just give me the code!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtfzNZaCFwNK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "961ffab3-d5bd-4040-d6c3-24c5a69e048b"
      },
      "source": [
        "weight = 0.5\n",
        "goal_pred = 0.8\n",
        "input = 0.5\n",
        "\n",
        "for i in range(20):\n",
        "  pred = input * weight\n",
        "  error = (pred - goal_pred) ** 2\n",
        "  delta = pred - goal_pred\n",
        "  weight_delta = delta * input   # Derivative(how fast the error changes, given changes in the weight)\n",
        "  weight = weight - weight_delta\n",
        "  print(f'Error: {str(error)} Prediction: {str(pred)}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error: 0.30250000000000005 Prediction: 0.25\n",
            "Error: 0.17015625000000004 Prediction: 0.3875\n",
            "Error: 0.095712890625 Prediction: 0.49062500000000003\n",
            "Error: 0.05383850097656251 Prediction: 0.56796875\n",
            "Error: 0.03028415679931642 Prediction: 0.6259765625\n",
            "Error: 0.0170348381996155 Prediction: 0.669482421875\n",
            "Error: 0.00958209648728372 Prediction: 0.70211181640625\n",
            "Error: 0.005389929274097089 Prediction: 0.7265838623046875\n",
            "Error: 0.0030318352166796153 Prediction: 0.7449378967285156\n",
            "Error: 0.0017054073093822882 Prediction: 0.7587034225463867\n",
            "Error: 0.0009592916115275371 Prediction: 0.76902756690979\n",
            "Error: 0.0005396015314842384 Prediction: 0.7767706751823426\n",
            "Error: 0.000303525861459885 Prediction: 0.7825780063867569\n",
            "Error: 0.00017073329707118678 Prediction: 0.7869335047900676\n",
            "Error: 9.603747960254256e-05 Prediction: 0.7902001285925507\n",
            "Error: 5.402108227642978e-05 Prediction: 0.7926500964444131\n",
            "Error: 3.038685878049206e-05 Prediction: 0.7944875723333098\n",
            "Error: 1.7092608064027242e-05 Prediction: 0.7958656792499823\n",
            "Error: 9.614592036015323e-06 Prediction: 0.7968992594374867\n",
            "Error: 5.408208020258491e-06 Prediction: 0.7976744445781151\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WErKxKX8I4VO",
        "colab_type": "text"
      },
      "source": [
        "Now that it works, let’s break it. Play around with the starting weight, goal_pred, and\n",
        "input numbers. You can set them all to just about anything, and the neural network will\n",
        "figure out how to predict the output given the input using the weight. See if you can find\n",
        "some combinations the neural network can’t predict. I find that trying to break something\n",
        "is a great way to learn about it.\n",
        "\n",
        "Let’s try setting input equal to 2, but still try to get the algorithm to predict 0.8."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CM8DhnyImui",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "4915aa41-c4ef-4598-9301-18980368f89b"
      },
      "source": [
        "weight = 0.5\n",
        "goal_pred = 0.8\n",
        "input = 2\n",
        "\n",
        "for i in range(20):\n",
        "  pred = input * weight\n",
        "  error = (pred - goal_pred) ** 2\n",
        "  delta = pred - goal_pred\n",
        "  weight_delta = delta * input   # Derivative(how fast the error changes, given changes in the weight)\n",
        "  weight = weight - weight_delta\n",
        "  print(f'Error: {str(error)} Prediction: {str(pred)}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error: 0.03999999999999998 Prediction: 1.0\n",
            "Error: 0.3599999999999998 Prediction: 0.20000000000000018\n",
            "Error: 3.2399999999999984 Prediction: 2.5999999999999996\n",
            "Error: 29.159999999999986 Prediction: -4.599999999999999\n",
            "Error: 262.4399999999999 Prediction: 16.999999999999996\n",
            "Error: 2361.959999999998 Prediction: -47.79999999999998\n",
            "Error: 21257.639999999978 Prediction: 146.59999999999994\n",
            "Error: 191318.75999999983 Prediction: -436.5999999999998\n",
            "Error: 1721868.839999999 Prediction: 1312.9999999999995\n",
            "Error: 15496819.559999991 Prediction: -3935.799999999999\n",
            "Error: 139471376.03999993 Prediction: 11810.599999999997\n",
            "Error: 1255242384.3599997 Prediction: -35428.59999999999\n",
            "Error: 11297181459.239996 Prediction: 106288.99999999999\n",
            "Error: 101674633133.15994 Prediction: -318863.79999999993\n",
            "Error: 915071698198.4395 Prediction: 956594.5999999997\n",
            "Error: 8235645283785.954 Prediction: -2869780.599999999\n",
            "Error: 74120807554073.56 Prediction: 8609344.999999996\n",
            "Error: 667087267986662.1 Prediction: -25828031.799999986\n",
            "Error: 6003785411879960.0 Prediction: 77484098.59999996\n",
            "Error: 5.403406870691965e+16 Prediction: -232452292.5999999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79amt5i4JQTM",
        "colab_type": "text"
      },
      "source": [
        "Whoa! That’s not what you want. The predictions exploded! They alternate from negative to\n",
        "positive and negative to positive, getting farther away from the true answer at every step. In\n",
        "other words, every update to the weight overcorrects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul1P_UG6JRPt",
        "colab_type": "text"
      },
      "source": [
        "## Visualizing the overcorrections"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K-WIrV-J8Ei",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "41f8f4c4-893e-4990-b4fc-269ca89ba1b6"
      },
      "source": [
        "weight = 0.5\n",
        "goal_pred = 0.8\n",
        "input = 2\n",
        "\n",
        "for i in range(20):\n",
        "  pred = input * weight\n",
        "  error = (pred - goal_pred) ** 2\n",
        "  delta = pred - goal_pred\n",
        "  weight_delta = delta * input   # Derivative(how fast the error changes, given changes in the weight)\n",
        "  weight = weight - weight_delta\n",
        "  print(f'Error: {str(error)} Prediction: {str(pred)} Weight: {str(weight)} Weight Delta: {str(weight_delta)} Delta: {str(delta)}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error: 0.03999999999999998 Prediction: 1.0 Weight: 0.10000000000000009 Weight Delta: 0.3999999999999999 Delta: 0.19999999999999996\n",
            "Error: 0.3599999999999998 Prediction: 0.20000000000000018 Weight: 1.2999999999999998 Weight Delta: -1.1999999999999997 Delta: -0.5999999999999999\n",
            "Error: 3.2399999999999984 Prediction: 2.5999999999999996 Weight: -2.2999999999999994 Weight Delta: 3.599999999999999 Delta: 1.7999999999999996\n",
            "Error: 29.159999999999986 Prediction: -4.599999999999999 Weight: 8.499999999999998 Weight Delta: -10.799999999999997 Delta: -5.399999999999999\n",
            "Error: 262.4399999999999 Prediction: 16.999999999999996 Weight: -23.89999999999999 Weight Delta: 32.39999999999999 Delta: 16.199999999999996\n",
            "Error: 2361.959999999998 Prediction: -47.79999999999998 Weight: 73.29999999999997 Weight Delta: -97.19999999999996 Delta: -48.59999999999998\n",
            "Error: 21257.639999999978 Prediction: 146.59999999999994 Weight: -218.2999999999999 Weight Delta: 291.59999999999985 Delta: 145.79999999999993\n",
            "Error: 191318.75999999983 Prediction: -436.5999999999998 Weight: 656.4999999999998 Weight Delta: -874.7999999999996 Delta: -437.3999999999998\n",
            "Error: 1721868.839999999 Prediction: 1312.9999999999995 Weight: -1967.8999999999994 Weight Delta: 2624.399999999999 Delta: 1312.1999999999996\n",
            "Error: 15496819.559999991 Prediction: -3935.799999999999 Weight: 5905.299999999998 Weight Delta: -7873.199999999998 Delta: -3936.599999999999\n",
            "Error: 139471376.03999993 Prediction: 11810.599999999997 Weight: -17714.299999999996 Weight Delta: 23619.599999999995 Delta: 11809.799999999997\n",
            "Error: 1255242384.3599997 Prediction: -35428.59999999999 Weight: 53144.49999999999 Weight Delta: -70858.79999999999 Delta: -35429.399999999994\n",
            "Error: 11297181459.239996 Prediction: 106288.99999999999 Weight: -159431.89999999997 Weight Delta: 212576.39999999997 Delta: 106288.19999999998\n",
            "Error: 101674633133.15994 Prediction: -318863.79999999993 Weight: 478297.2999999999 Weight Delta: -637729.1999999998 Delta: -318864.5999999999\n",
            "Error: 915071698198.4395 Prediction: 956594.5999999997 Weight: -1434890.2999999996 Weight Delta: 1913187.5999999994 Delta: 956593.7999999997\n",
            "Error: 8235645283785.954 Prediction: -2869780.599999999 Weight: 4304672.499999998 Weight Delta: -5739562.799999998 Delta: -2869781.399999999\n",
            "Error: 74120807554073.56 Prediction: 8609344.999999996 Weight: -12914015.899999993 Weight Delta: 17218688.39999999 Delta: 8609344.199999996\n",
            "Error: 667087267986662.1 Prediction: -25828031.799999986 Weight: 38742049.29999998 Weight Delta: -51656065.19999997 Delta: -25828032.599999987\n",
            "Error: 6003785411879960.0 Prediction: 77484098.59999996 Weight: -116226146.29999995 Weight Delta: 154968195.59999993 Delta: 77484097.79999997\n",
            "Error: 5.403406870691965e+16 Prediction: -232452292.5999999 Weight: 348678440.4999999 Weight Delta: -464904586.79999983 Delta: -232452293.39999992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpbgmKUBJfZg",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/derivative-4.JPG?raw=1' width='800'/>\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/derivative-5.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF6QS80PKOWd",
        "colab_type": "text"
      },
      "source": [
        "## Divergence\n",
        "**Sometimes neural networks explode in value. Oops?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA1oxghNKWSs",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/derivative-6.JPG?raw=1' width='800'/>\n",
        "\n",
        "What really happened? The explosion in the error was caused by the fact that you made the\n",
        "input larger. Consider how you’re updating the weight:\n",
        "\n",
        "$$weight = weight - (input * (pred - goal_p))$$\n",
        "\n",
        "If the input is sufficiently large, this can make the weight update large even when the error is\n",
        "small. What happens when you have a large weight update and a small error? The network\n",
        "overcorrects. If the new error is even bigger, the network overcorrects even more. This\n",
        "causes the phenomenon you saw earlier, called divergence.\n",
        "\n",
        "If you have a big input, the prediction is very sensitive to changes in the weight (because\n",
        "pred = input * weight). This can cause the network to overcorrect. In other words, even\n",
        "though the weight is still starting at 0.5, the derivative at that point is very steep. See how\n",
        "tight the U-shaped error curve is in the graph?\n",
        "\n",
        "This is really intuitive. How do you predict? By multiplying the input by the weight. So, if the\n",
        "input is huge, small changes in the weight will cause changes in the prediction. The error is\n",
        "very sensitive to the weight. In other words, the derivative is really big. How do you make\n",
        "it smaller?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2t5wAJxLMTB",
        "colab_type": "text"
      },
      "source": [
        "## Introducing alpha\n",
        "**It’s the simplest way to prevent overcorrecting weight updates.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsUwMkKMLRqR",
        "colab_type": "text"
      },
      "source": [
        "What’s the problem you’re trying to solve? That if the input is too big, then the weight\n",
        "update can overcorrect. What’s the symptom? That when you overcorrect, the new\n",
        "derivative is even larger in magnitude than when you started (although the sign will be\n",
        "the opposite).\n",
        "\n",
        "The symptom is this overshooting. The solution is to multiply the weight update by a\n",
        "fraction to make it smaller. In most cases, this involves multiplying the weight update\n",
        "by a single real-valued number between 0 and 1, known as alpha. Note: this has no\n",
        "effect on the core issue, which is that the input is larger. It will also reduce the weight\n",
        "updates for inputs that aren’t too large.\n",
        "\n",
        "Finding the appropriate alpha, even for state-of-the-art neural networks, is often done\n",
        "by guessing. You watch the error over time. If it starts diverging (going up), then the\n",
        "alpha is too high, and you decrease it. If learning is happening too slowly, then the alpha\n",
        "is too low, and you increase it. There are other methods than simple gradient descent\n",
        "that attempt to counter for this, but gradient descent is still very popular."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOdxiWCYrVVO",
        "colab_type": "text"
      },
      "source": [
        "### Alpha in code\n",
        "**Where does our “alpha” parameter come into play?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIfC-qmrrbcL",
        "colab_type": "text"
      },
      "source": [
        "You just learned that alpha reduces the weight update so it doesn’t overshoot. How does this\n",
        "affect the code? Well, you were updating the weights according to the following formula:\n",
        "\n",
        "$$weight = weight - derivative$$\n",
        "\n",
        "Accounting for alpha is a rather small change, as shown next. Notice that if alpha is\n",
        "small (say, 0.01), it will reduce the weight update considerably, thus preventing it from\n",
        "overshooting:\n",
        "\n",
        "$$weight = weight - (alpha * derivative)$$\n",
        "\n",
        "That was easy. Let’s install alpha into the tiny implementation from the beginning of this\n",
        "chapter and run it where input = 2 (which previously didn’t work):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lNwPNFVJFh0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "8bdf760e-1313-48a4-cf12-75b1623bee94"
      },
      "source": [
        "weight = 0.5\n",
        "goal_pred = 0.8\n",
        "input = 2\n",
        "alpha = 0.1   # What happens when you make alpha crazy small or big? What about making it negative?\n",
        "\n",
        "for i in range(20):\n",
        "  pred = input * weight\n",
        "  error = (pred - goal_pred) ** 2\n",
        "  derivative = input * (pred - goal_pred)  #  Derivative(how fast the error changes, given changes in the weight)\n",
        "  weight = weight - (alpha * derivative)\n",
        "  print(f'Error: {str(error)} Prediction: {str(pred)}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error: 0.03999999999999998 Prediction: 1.0\n",
            "Error: 0.0144 Prediction: 0.92\n",
            "Error: 0.005183999999999993 Prediction: 0.872\n",
            "Error: 0.0018662400000000014 Prediction: 0.8432000000000001\n",
            "Error: 0.0006718464000000028 Prediction: 0.8259200000000001\n",
            "Error: 0.00024186470400000033 Prediction: 0.815552\n",
            "Error: 8.70712934399997e-05 Prediction: 0.8093312\n",
            "Error: 3.134566563839939e-05 Prediction: 0.80559872\n",
            "Error: 1.1284439629823931e-05 Prediction: 0.803359232\n",
            "Error: 4.062398266736526e-06 Prediction: 0.8020155392\n",
            "Error: 1.4624633760252567e-06 Prediction: 0.8012093235200001\n",
            "Error: 5.264868153690924e-07 Prediction: 0.8007255941120001\n",
            "Error: 1.8953525353291194e-07 Prediction: 0.8004353564672001\n",
            "Error: 6.82326912718715e-08 Prediction: 0.8002612138803201\n",
            "Error: 2.456376885786678e-08 Prediction: 0.8001567283281921\n",
            "Error: 8.842956788836216e-09 Prediction: 0.8000940369969153\n",
            "Error: 3.1834644439835434e-09 Prediction: 0.8000564221981492\n",
            "Error: 1.1460471998340758e-09 Prediction: 0.8000338533188895\n",
            "Error: 4.125769919393652e-10 Prediction: 0.8000203119913337\n",
            "Error: 1.485277170987127e-10 Prediction: 0.8000121871948003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dsa78e7s0nh",
        "colab_type": "text"
      },
      "source": [
        "Voilà! The tiniest neural network can now make good predictions again. How did I\n",
        "know to set alpha to 0.1? To be honest, I tried it, and it worked. And despite all the crazy\n",
        "advancements of deep learning in the past few years, most people just try several orders of\n",
        "magnitude of alpha (10, 1, 0.1, 0.01, 0.001, 0.0001) and then tweak it from there to see what\n",
        "works best. It’s more art than science."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhnw38Ths1ys",
        "colab_type": "text"
      },
      "source": [
        "## Memorizing\n",
        "**It’s time to really learn this stuff.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S6fHcUis9BN",
        "colab_type": "text"
      },
      "source": [
        "Why does this work? Well, for starters, the only way to know you’ve gleaned all the\n",
        "information necessary from this chapter is to try to produce it from your head. Neural\n",
        "networks have lots of small moving parts, and it’s easy to miss one.\n",
        "\n",
        "Why is this important for the rest of the book? In the following chapters, I’ll be referring to\n",
        "the concepts discussed in this chapter at a faster pace so that I can spend plenty of time on\n",
        "the newer material. It’s vitally important that when I say something like “Add your alpha\n",
        "parameterization to the weight update,” you immediately recognize which concepts from\n",
        "this chapter I’m referring to.\n",
        "\n",
        "All that is to say, memorizing small bits of neural network code has been hugely beneficial\n",
        "for me personally, as well as for many individuals who have taken my advice on this subject\n",
        "in the past."
      ]
    }
  ]
}