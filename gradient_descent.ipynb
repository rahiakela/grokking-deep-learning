{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gradient-descent.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/grokking-deep-learning/blob/4-gradient-descent/gradient_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUDd4oT1agdf",
        "colab_type": "text"
      },
      "source": [
        "# introduction to neural learning: gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZB9oWbPEbGC_",
        "colab_type": "text"
      },
      "source": [
        "## Predict, compare, and learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TcELK-6axYM",
        "colab_type": "text"
      },
      "source": [
        "We learned about the paradigm “predict, compare, learn,” and we dove\n",
        "deep into the first step: **predict**. So now we cover the next two steps of the paradigm: **compare and learn**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRNR9rlmbMYC",
        "colab_type": "text"
      },
      "source": [
        "### Compare\n",
        "\n",
        "**Comparing gives a measurement of how much a prediction\n",
        "“missed” by.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xJekwbfbNd8",
        "colab_type": "text"
      },
      "source": [
        "Once you’ve made a prediction, the next step is to evaluate how well you did. This may\n",
        "seem like a simple concept, but you’ll find that coming up with a good way to measure\n",
        "error is one of the most important and complicated subjects of deep learning.\n",
        "\n",
        "You’ll also learn that error is always positive! We’ll consider the analogy of an\n",
        "archer hitting a target: whether the shot is too low by an inch or too high by an inch, the\n",
        "error is still just 1 inch. \n",
        "\n",
        "In the neural network compare step, you need to consider these\n",
        "kinds of properties when measuring error.\n",
        "\n",
        "we evaluate only one simple way of measuring error: mean\n",
        "squared error. It’s but one of many ways to evaluate the accuracy of a neural network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caHQp839cazu",
        "colab_type": "text"
      },
      "source": [
        "### Learn\n",
        "**Learning tells each weight how it can change to reduce the error.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM5b6IYVcgYv",
        "colab_type": "text"
      },
      "source": [
        "Learning is all about error attribution, or the art of figuring out how each weight played its part in creating error. It’s the blame game of deep learning.\n",
        "\n",
        "So we’ll spend times looking at the most popular version of the deep learning blame game:**gradient descent**.\n",
        "\n",
        "At the end of the day, it results in computing a number for each weight. That number\n",
        "represents how that weight should be higher or lower in order to reduce the error. Then\n",
        "you’ll move the weight according to that number, and you’ll be finished."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9ZNdXuEdfWQ",
        "colab_type": "text"
      },
      "source": [
        "## Compare: Does your network make good predictions?\n",
        "**Let’s measure the error and find out!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBiPsJJid01W",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/measure-error-1.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp4zZfBakuLx",
        "colab_type": "code",
        "outputId": "2318f8ee-f0aa-426e-c7b1-b782d94c76bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "knob_weight = 0.5\n",
        "input = 0.5\n",
        "goal_pred = 0.8\n",
        "\n",
        "pred = input * knob_weight\n",
        "\n",
        "error = (pred - goal_pred) ** 2\n",
        "print(error)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.30250000000000005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hzZyZULpDgK",
        "colab_type": "text"
      },
      "source": [
        "## Why measure error?\n",
        "**Measuring error simplifies the problem.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBPiJ5ZwpHk9",
        "colab_type": "text"
      },
      "source": [
        "The goal of training a neural network is to make correct predictions. That’s what you want.\n",
        "And in the most pragmatic world you want the\n",
        "network to take input that you can easily calculate (today’s stock price) and predict things that\n",
        "are hard to calculate (tomorrow’s stock price). That’s what makes a neural network useful.\n",
        "\n",
        "It turns out that changing knob_weight to make the network correctly predict\n",
        "goal_prediction is slightly more complicated than changing knob_weight to make\n",
        "error == 0. There’s something more concise about looking at the problem this way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAk1f-AKlG_1",
        "colab_type": "code",
        "outputId": "90cd2a23-ecc9-469c-fb82-fe4b2ebe1cee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "knob_weight = 0.9\n",
        "input = 0.5\n",
        "goal_pred = 0.8\n",
        "\n",
        "pred = input * knob_weight\n",
        "\n",
        "error = (pred - goal_pred) ** 2\n",
        "print(error)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.12250000000000003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPzYLlJbroN7",
        "colab_type": "text"
      },
      "source": [
        "**Different ways of measuring error prioritize error differently.**\n",
        "\n",
        "By squaring the error, numbers that are less than 1 get smaller, whereas numbers that are greater than 1 get bigger. You’re going to change what I call pure error (pred - goal_pred) so that bigger errors become very big and smaller errors quickly become irrelevant.\n",
        "\n",
        "By measuring error this way, you can prioritize big errors over smaller ones. When you have\n",
        "somewhat large pure errors (say, 10), you’ll tell yourself that you have very large error $$(10**2 == 100)$$ and in contrast, when you have small pure errors (say, 0.01), you’ll tell yourself that you\n",
        "have very small error $$(0.01**2 == 0.0001)$$ See what I mean about prioritizing? It’s just modifying\n",
        "what you consider to be error so that you amplify big ones and largely ignore small ones.\n",
        "\n",
        "In contrast, if you took the absolute value instead of squaring the error, you wouldn’t have this\n",
        "type of prioritization. The error would just be the positive version of the pure error—which\n",
        "would be fine, but different.\n",
        "\n",
        "**Why do you want only positive error?**\n",
        "\n",
        "Eventually, you’ll be working with millions of input -> goal_prediction pairs, and we’ll\n",
        "still want to make accurate predictions. So, you’ll try to take the average error down to 0.\n",
        "\n",
        "This presents a problem if the error can be positive and negative.\n",
        "\n",
        "Imagine if you were\n",
        "trying to get the neural network to correctly predict two datapoints—two input ->\n",
        "goal_prediction pairs. \n",
        "\n",
        "If the first had an error of 1,000 and the second had an error of\n",
        "–1,000, then the average error would be zero! \n",
        "\n",
        "You’d fool yourself into thinking you predicted\n",
        "perfectly, when you missed by 1,000 each time! That would be really bad. \n",
        "\n",
        "Thus, you want the\n",
        "error of each prediction to always be positive so they don’t accidentally cancel each other out\n",
        "when you average them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbEwXrLhuF4T",
        "colab_type": "text"
      },
      "source": [
        "## What’s the simplest form of neural learning?\n",
        "\n",
        "**Learning using the hot and cold method.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcLpn5QjuOTu",
        "colab_type": "text"
      },
      "source": [
        "At the end of the day, learning is really about one thing: adjusting knob_weight either up\n",
        "or down so the error is reduced. If you keep doing this and the error goes to 0, you’re done\n",
        "learning! How do you know whether to turn the knob up or down? Well, you try both up and\n",
        "down and see which one reduces the error! Whichever one reduces the error is used to update\n",
        "knob_weight. It’s simple but effective. After you do this over and over again, eventually\n",
        "error == 0, which means the neural network is predicting with perfect accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mlDgKaiubhk",
        "colab_type": "text"
      },
      "source": [
        "### Hot and cold learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDMM64KOucnK",
        "colab_type": "text"
      },
      "source": [
        "Hot and cold learning means wiggling the weights to see which direction reduces the error\n",
        "the most, moving the weights in that direction, and repeating until the error gets to 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LRlku00xHeA",
        "colab_type": "text"
      },
      "source": [
        "### An empty network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twDuGG0swbFT",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hot-and-cold-learning-1.JPG?raw=1' width='800'/>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3BvEp4UrUic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight = 0.1\n",
        "lr = 0.01\n",
        "\n",
        "def neural_network(input, weight):\n",
        "  prediction = input * weight\n",
        "  return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmDB-dA7xLyn",
        "colab_type": "text"
      },
      "source": [
        "### PREDICT: Making a prediction and evaluating error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mo9OfQnxQrH",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hot-and-cold-learning-2.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQg5DMCCw_2v",
        "colab_type": "code",
        "outputId": "9a3ccf7c-23f1-41c2-b074-8d8ce18465c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "number_of_toes = [8.5]\n",
        "win_or_lose_binary = [1] # (won!!!)\n",
        "\n",
        "input = number_of_toes[0]\n",
        "true = win_or_lose_binary[0]\n",
        "\n",
        "pred = neural_network(input, weight)\n",
        "\n",
        "error = (pred - true) ** 2\n",
        "print(error)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.022499999999999975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee3remxlx7lx",
        "colab_type": "text"
      },
      "source": [
        "### COMPARE: Making a prediction with a higher weight and evaluating error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6X_MhDCx8to",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hot-and-cold-learning-3.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3Wz18CVxxYl",
        "colab_type": "code",
        "outputId": "eeeaf9e2-788e-4da9-bfcb-ca18c1242572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "lr = 0.1 # higher\n",
        "\n",
        "pred_up = neural_network(input, weight + lr)\n",
        "\n",
        "err_up = (pred_up - true) ** 2\n",
        "print(err_up)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.49000000000000027\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEsEMaH2yZeu",
        "colab_type": "code",
        "outputId": "b3e99946-2791-41ae-c1ad-1c626da2a912",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "lr = 0.01  # lower\n",
        "\n",
        "pred_down = neural_network(input, weight - lr)\n",
        "\n",
        "err_down = (pred_down - true) ** 2\n",
        "print(err_down)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.05522499999999994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5-vV9BuzJXM",
        "colab_type": "text"
      },
      "source": [
        "### COMPARE + LEARN: Comparing the errors and setting the new weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fADcUTRgzKvI",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hot-and-cold-learning-4.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZABEiCOy3tY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if (error > err_down or error > err_up):\n",
        "  if ( err_down < err_up):\n",
        "    weight -= lr\n",
        "  if (err_up < err_up):\n",
        "    weight += lr  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9DteFxa0SsN",
        "colab_type": "text"
      },
      "source": [
        "This reveals what learning in neural networks really is: a search problem. You’re searching\n",
        "for the best possible configuration of weights so the network’s error falls to 0 (and predicts\n",
        "perfectly). As with all other forms of search, you might not find exactly what you’re looking\n",
        "for, and even if you do, it may take some time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fanpAjs0V-F",
        "colab_type": "text"
      },
      "source": [
        "## Hot and cold learning\n",
        "**This is perhaps the simplest form of learning.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k24XTFJN3MvR",
        "colab_type": "text"
      },
      "source": [
        "#### Complete Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTqXlYC6zspO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight = 0.5\n",
        "input = 0.5\n",
        "\n",
        "goal_prediction = 0.8\n",
        "\n",
        "step_amount = 0.001  # How much to move the weights each iteration\n",
        "\n",
        "# Repeat learning many times so the error can keep getting smaller.\n",
        "for iteration in range(1101):\n",
        "  prediction = input * weight\n",
        "  error = (prediction - goal_prediction) ** 2\n",
        "\n",
        "  print(f'Error: {str(error)}\\t\\tPrediction: {str(prediction)}')\n",
        "\n",
        "  up_prediction = input * (weight + step_amount)   # try up!\n",
        "  up_error = (goal_prediction - up_prediction) ** 2\n",
        "\n",
        "  down_prediction = input * (weight - step_amount)  # try down!\n",
        "  down_error = (goal_prediction - down_prediction) ** 2\n",
        "\n",
        "  if (down_error < up_error):\n",
        "    weight = weight - step_amount    # If down is better, go down!\n",
        "  if (down_error > up_error):\n",
        "    weight = weight + step_amount    # If up is better, go up!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4QV-UGs3GDi",
        "colab_type": "text"
      },
      "source": [
        "### Characteristics of hot and cold learning\n",
        "**It’s simple.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f7tmYik4Fi2",
        "colab_type": "text"
      },
      "source": [
        "Hot and cold learning is simple. After making a prediction, you predict two more times, once with a\n",
        "slightly higher weight and again with a slightly lower weight. You then move weight depending on\n",
        "which direction gave a smaller error. Repeating this enough times eventually reduces error to 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqMA2Bo3_0OT",
        "colab_type": "text"
      },
      "source": [
        "####Problem 1: It’s inefficient.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4ReA-jJ_8pL",
        "colab_type": "text"
      },
      "source": [
        "You have to predict multiple times to make a single knob_weight update. This seems very inefficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Spz5XK-LAAqA",
        "colab_type": "text"
      },
      "source": [
        "#### Problem 2: Sometimes it’s impossible to predict the exact goal prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U58BXsGHAFJC",
        "colab_type": "text"
      },
      "source": [
        "With a set step_amount, unless the perfect weight is exactly n*step_amount away, the network\n",
        "will eventually overshoot by some number less than step_amount. When it does, it will then\n",
        "start alternating back and forth between each side of goal_prediction. Set step_amount to 0.2\n",
        "to see this in action. If you set step_amount to 10, you’ll really break it. When I try this, I see the\n",
        "following output.\n",
        "\n",
        "It never remotely comes close to 0.8!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqEx5rq72heo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight = 0.5\n",
        "input = 0.5\n",
        "\n",
        "goal_prediction = 0.8\n",
        "\n",
        "step_amount = 0.2  # How much to move the weights each iteration\n",
        "\n",
        "# Repeat learning many times so the error can keep getting smaller.\n",
        "for iteration in range(1101):\n",
        "  prediction = input * weight\n",
        "  error = (prediction - goal_prediction) ** 2\n",
        "\n",
        "  print(f'Error: {str(error)}\\t\\tPrediction: {str(prediction)}')\n",
        "\n",
        "  up_prediction = input * (weight + step_amount)   # try up!\n",
        "  up_error = (goal_prediction - up_prediction) ** 2\n",
        "\n",
        "  down_prediction = input * (weight - step_amount)  # try down!\n",
        "  down_error = (goal_prediction - down_prediction) ** 2\n",
        "\n",
        "  if (down_error < up_error):\n",
        "    weight = weight - step_amount    # If down is better, go down!\n",
        "  if (down_error > up_error):\n",
        "    weight = weight + step_amount    # If up is better, go up!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK0e2sORAwSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight = 0.5\n",
        "input = 0.5\n",
        "\n",
        "goal_prediction = 0.8\n",
        "\n",
        "step_amount = 10  # How much to move the weights each iteration\n",
        "\n",
        "# Repeat learning many times so the error can keep getting smaller.\n",
        "for iteration in range(1101):\n",
        "  prediction = input * weight\n",
        "  error = (prediction - goal_prediction) ** 2\n",
        "\n",
        "  print(f'Error: {str(error)}\\t\\tPrediction: {str(prediction)}')\n",
        "\n",
        "  up_prediction = input * (weight + step_amount)   # try up!\n",
        "  up_error = (goal_prediction - up_prediction) ** 2\n",
        "\n",
        "  down_prediction = input * (weight - step_amount)  # try down!\n",
        "  down_error = (goal_prediction - down_prediction) ** 2\n",
        "\n",
        "  if (down_error < up_error):\n",
        "    weight = weight - step_amount    # If down is better, go down!\n",
        "  if (down_error > up_error):\n",
        "    weight = weight + step_amount    # If up is better, go up!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtTWsGALBCE7",
        "colab_type": "text"
      },
      "source": [
        "The real problem is that even though you know the correct direction to move weight, you don’t know\n",
        "the correct amount. Instead, you pick a fixed one at random (step_amount). Furthermore, this amount\n",
        "has nothing to do with error. Whether error is big or tiny, step_amount is the same. \n",
        "\n",
        "So, hot and cold\n",
        "learning is kind of a bummer. It’s inefficient because you predict three times for each weight update, and\n",
        "step_ amount is arbitrary, which can prevent you from learning the correct weight value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE5lEN17BYQp",
        "colab_type": "text"
      },
      "source": [
        "## Calculating both direction and amount from error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_Cs4gdi4ksu",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/direction-and-amount.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOBsvbKSA2pA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight = 0.5\n",
        "goal_pred = 0.8\n",
        "input = 0.5\n",
        "\n",
        "for iteration in range(20):\n",
        "  pred = input * weight\n",
        "  error = (pred - goal_pred) ** 2\n",
        "  direction_and_amount = (pred - goal_pred) * input\n",
        "  weight = weight - direction_and_amount\n",
        "  print(f'Error: {str(error)} \\t\\t\\t Prediction: {str(pred)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDUjafU16FaR",
        "colab_type": "text"
      },
      "source": [
        "## One iteration of gradient descent\n",
        "\n",
        "**This performs a weight update on a single training example\n",
        "(input->true) pair.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK14-kTv6HQU",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/gradient-descent-1.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFXKwLId5l8S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight = 0.1\n",
        "alpha = 0.01\n",
        "\n",
        "def neural_network(input , weight):\n",
        "  prediction = input * weight\n",
        "  return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebewGj6L6lPB",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/gradient-descent-2.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxWs4TZF6i3g",
        "colab_type": "code",
        "outputId": "e7388ce4-1a92-434d-89c1-11050af79732",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "number_of_toes = [8.5]\n",
        "win_or_lose_binary = [1]  # (won!!!)\n",
        "\n",
        "input = number_of_toes[0]\n",
        "goal_pred = win_or_lose_binary[0]\n",
        "\n",
        "pred = neural_network(input, weight)\n",
        "\n",
        "error = (pred - goal_pred) ** 2\n",
        "print(error)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.022499999999999975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKoX3J_t7om2",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/gradient-descent-3.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "behmF1Si7m6t",
        "colab_type": "code",
        "outputId": "a4dfed0c-17ef-4f7a-9feb-75901deca5fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "number_of_toes = [8.5]\n",
        "win_or_lose_binary = [1]  # (won!!!)\n",
        "\n",
        "input = number_of_toes[0]\n",
        "goal_pred = win_or_lose_binary[0]\n",
        "\n",
        "pred = neural_network(input, weight)\n",
        "print(pred)\n",
        "\n",
        "delta = pred - goal_pred\n",
        "print(delta)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8500000000000001\n",
            "-0.1499999999999999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpxnymUb8wr5",
        "colab_type": "text"
      },
      "source": [
        "delta is a measurement of how much this node missed. The true prediction is 1.0, and the network’s prediction was 0.85, so the network was too low by 0.15. Thus, delta is negative 0.15.\n",
        "\n",
        "The primary difference between gradient descent and this implementation is the new variable\n",
        "delta. It’s the raw amount that the node was too high or too low. Instead of computing\n",
        "direction_and_amount directly, you first calculate how much you want the output node to be\n",
        "different. Only then do you compute direction_and_amount to change weight (in step 4, now\n",
        "renamed weight_delta):\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/gradient-descent-4.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAvZ06XT8PsH",
        "colab_type": "code",
        "outputId": "13641527-f12d-4dfd-c3a9-0f062bcd9bd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "number_of_toes = [8.5]\n",
        "win_or_lose_binary = [1]  # (won!!!)\n",
        "\n",
        "input = number_of_toes[0]\n",
        "goal_pred = win_or_lose_binary[0]\n",
        "\n",
        "pred = neural_network(input, weight)\n",
        "print(pred)\n",
        "\n",
        "error = (pred - goal_pred) ** 2\n",
        "print(error)\n",
        "\n",
        "delta = pred - goal_pred\n",
        "print(delta)\n",
        "\n",
        "weight_delta = input * delta\n",
        "print(weight_delta)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8500000000000001\n",
            "0.022499999999999975\n",
            "-0.1499999999999999\n",
            "-1.2749999999999992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du_MJ2E-99WP",
        "colab_type": "text"
      },
      "source": [
        "weight_delta is a measure of how much a weight caused the network to miss. You calculate\n",
        "it by multiplying the weight’s output node delta by the weight’s input. Thus, you create\n",
        "each weight_delta by scaling its output node delta by the weight’s input. This accounts\n",
        "for the three aforementioned properties of direction_and_amount: scaling, negative\n",
        "reversal, and stopping.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/gradient-descent-6.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GhLKG9S-LkZ",
        "colab_type": "code",
        "outputId": "47fb1979-256d-4683-e587-63745fe3205a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "number_of_toes = [8.5]\n",
        "win_or_lose_binary = [1]  # (won!!!)\n",
        "\n",
        "input = number_of_toes[0]\n",
        "goal_pred = win_or_lose_binary[0]\n",
        "\n",
        "pred = neural_network(input, weight)\n",
        "print(pred)\n",
        "\n",
        "error = (pred - goal_pred) ** 2\n",
        "print(error)\n",
        "\n",
        "delta = pred - goal_pred\n",
        "print(delta)\n",
        "\n",
        "weight_delta = input * delta\n",
        "print(weight_delta)\n",
        "\n",
        "alpha = 0.01  # learning rate\n",
        "weight -= weight_delta * alpha\n",
        "print(weight)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8500000000000001\n",
            "0.022499999999999975\n",
            "-0.1499999999999999\n",
            "-1.2749999999999992\n",
            "0.11275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg2ytqDT_AW7",
        "colab_type": "text"
      },
      "source": [
        "You multiply weight_delta by a small number alpha before using it to update weight. This lets you control how fast the network learns. If it learns too fast, it can update weights too aggressively and overshoot.\n",
        "\n",
        "Note that the weight update made the same change (small increase) as hot and cold learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf_ZfCby_S6e",
        "colab_type": "text"
      },
      "source": [
        "## Learning is just reducing error\n",
        "**You can modify weight to reduce error.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIar_Q_D-tn9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "b661bbc9-c9e0-496a-c933-9218c3e46add"
      },
      "source": [
        "weight, goal_pred, input = (0.0, 0.8, 0.5)\n",
        "\n",
        "for i in range(4):\n",
        "  # These lines have a secret.\n",
        "  pred = input * weight             # multiply weight parameter and input feature\n",
        "  error = (pred - goal_pred) ** 2   # measure the diffrerence of predicted value and target value\n",
        "\n",
        "  delta = pred - goal_pred          # get the diffrerence(missing value) of predicted value and target value\n",
        "  weight_delta = input * delta      # multiply again input feature by the diffrerence(missing value)\n",
        "  weight = weight - weight_delta    # adjust the weight by subtracting with multiplied input feature and the diffrerence(missing value)\n",
        "  print(f'Error: {str(error)} \\t\\t Prediction: {str(pred)}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error: 0.6400000000000001 \t\t Prediction: 0.0\n",
            "Error: 0.3600000000000001 \t\t Prediction: 0.2\n",
            "Error: 0.2025 \t\t Prediction: 0.35000000000000003\n",
            "Error: 0.11390625000000001 \t\t Prediction: 0.4625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LVLn3WN5A9n",
        "colab_type": "text"
      },
      "source": [
        "**This approach adjusts each weight in the correct direction and by the correct amount so that error reduces to 0.**\n",
        "\n",
        "All you’re trying to do is figure out the right direction and amount to modify weight so that\n",
        "error goes down. The secret lies in the pred and error calculations. Notice that you use pred\n",
        "inside the error calculation.\n",
        "\n",
        "Let’s replace the pred variable with the code used to generate it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsbOQBE82d_W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "73a171cd-5597-49d8-ec99-7543e4d46ecc"
      },
      "source": [
        "error = ((input * weight) - goal_pred) ** 2\n",
        "print(error)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.06407226562500003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtPW8CZl5m6a",
        "colab_type": "text"
      },
      "source": [
        "This doesn’t change the value of error at all! It just combines the two lines of code and computes error directly.\n",
        "\n",
        "Remember that input and goal_prediction are fixed at 0.5 and\n",
        "0.8, respectively (you set them before the network starts training). \n",
        "\n",
        "So, if you replace their\n",
        "variables names with the values, the secret becomes clear:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6HpuKl96DDO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e5e9bcbc-f2b8-400a-ed51-a7b6ffff6f92"
      },
      "source": [
        "error = ((0.5 * weight) - 0.8) ** 2\n",
        "print(error)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.06407226562500003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Mtf43Ks6OIQ",
        "colab_type": "text"
      },
      "source": [
        "Let’s say you increased weight by 0.5. If there’s an exact relationship between error and weight,\n",
        "you should be able to calculate how much this also moves error. What if you wanted to move\n",
        "error in a specific direction? Could it be done?\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/reduce-error-1.JPG?raw=1' width='800'/>\n",
        "\n",
        "This graph represents every value of error for every weight according to the relationship in the\n",
        "previous formula. Notice it makes a nice bowl shape. The black dot is at the point of both the\n",
        "current weight and error. The dotted circle is where you want to be (error == 0).\n",
        "\n",
        "**The slope points to the bottom of the bowl (lowest error) no matter where you are in the\n",
        "bowl. You can use this slope to help the neural network reduce the error.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdxuQALi7EgX",
        "colab_type": "text"
      },
      "source": [
        "## Let’s watch several steps of learning\n",
        "\n",
        "**Will we eventually find the bottom of the bowl?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ErgKcAj7924",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "cb6eaa82-9d62-49ad-8cdb-7b641f1b31ce"
      },
      "source": [
        "weight, goal_pred, input = (0.0, 0.8, 1.1)\n",
        "\n",
        "for i in range(4):\n",
        "\n",
        "  print(f'-----------\\nWeight: {str(weight)}')\n",
        "\n",
        "  # These lines have a secret.\n",
        "  pred = input * weight             # multiply weight parameter and input feature\n",
        "  error = (pred - goal_pred) ** 2   # measure the diffrerence of predicted value and target value\n",
        "\n",
        "  delta = pred - goal_pred          # get the diffrerence(missing value) of predicted value and target value\n",
        "  weight_delta = input * delta      # multiply again input feature by the diffrerence(missing value)\n",
        "  weight = weight - weight_delta    # adjust the weight by subtracting with multiplied input feature and the diffrerence(missing value)\n",
        "  print(f'Error: {str(error)}  Prediction: {str(pred)}')\n",
        "  print(f'Delta: {str(delta)}  Weight Delta: {str(weight_delta)}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------\n",
            "Weight: 0.0\n",
            "Error: 0.6400000000000001  Prediction: 0.0\n",
            "Delta: -0.8  Weight Delta: -0.8800000000000001\n",
            "-----------\n",
            "Weight: 0.8800000000000001\n",
            "Error: 0.02822400000000005  Prediction: 0.9680000000000002\n",
            "Delta: 0.16800000000000015  Weight Delta: 0.1848000000000002\n",
            "-----------\n",
            "Weight: 0.6951999999999999\n",
            "Error: 0.0012446784000000064  Prediction: 0.76472\n",
            "Delta: -0.03528000000000009  Weight Delta: -0.0388080000000001\n",
            "-----------\n",
            "Weight: 0.734008\n",
            "Error: 5.4890317439999896e-05  Prediction: 0.8074088\n",
            "Delta: 0.007408799999999993  Weight Delta: 0.008149679999999992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDKW4oOu8cGq",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/reduce-error-2.JPG?raw=1' width='800'/>\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/reduce-error-3.JPG?raw=1' width='800'/>\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/reduce-error-4.JPG?raw=1' width='800'/>\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/reduce-error-5.JPG?raw=1' width='800'/>\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/reduce-error-6.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-x9MrYvAjW-",
        "colab_type": "text"
      },
      "source": [
        "## Why does this work? What is weight_delta, really?\n",
        "\n",
        "**Let’s back up and talk about functions. What is a function?\n",
        "How do you understand one?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_Uq8LfkCEKR",
        "colab_type": "text"
      },
      "source": [
        "Consider this function:\n",
        "\n",
        "def my_function(x):\n",
        "   return x * 2\n",
        "\n",
        "Every function has what you might call moving parts: pieces you can tweak or change to make\n",
        "the output the function generates different. Consider my_function in the previous example. Ask\n",
        "yourself, “What’s controlling the relationship between the input and the output of this function?”\n",
        "The answer is, the 2. Ask the same question about the following function:\n",
        "\n",
        "$$error = ((input * weight) - goal_pred) ** 2$$\n",
        "\n",
        "What’s controlling the relationship between input and the output (error)? Plenty of things\n",
        "are—this function is a bit more complicated! goal_pred, input, **2, weight, and all the\n",
        "parentheses and algebraic operations (addition, subtraction, and so on) play a part in calculating\n",
        "the error. Tweaking any one of them would change the error. This is important to consider.\n",
        "\n",
        "As a thought exercise, consider changing goal_pred to reduce the error. This is silly, but totally\n",
        "doable. In life, you might call this (setting goals to be whatever your capability is) “giving up.”\n",
        "You’re denying that you missed! That wouldn’t do.\n",
        "\n",
        "What if you changed input until error went to 0? Well, that’s akin to seeing the world as you\n",
        "want to see it instead of as it actually is. You’re changing the input data until you’re predicting\n",
        "what you want to predict (this is loosely how inceptionism works).\n",
        "\n",
        "Now consider changing the 2, or the additions, subtractions, or multiplications. This is just\n",
        "changing how you calculate error in the first place. The error calculation is meaningless if\n",
        "it doesn’t actually give a good measure of how much you missed (with the right properties\n",
        "mentioned a few pages ago). This won’t do, either.\n",
        "\n",
        "What’s left? The only variable remaining is weight. Adjusting it doesn’t change your perception\n",
        "of the world, doesn’t change your goal, and doesn’t destroy your error measure. Changing\n",
        "weight means the function conforms to the patterns in the data. By forcing the rest of the\n",
        "function to be unchanging, you force the function to correctly model some pattern in the data.\n",
        "It’s only allowed to modify how the network predicts.\n",
        "\n",
        "To sum up: you modify specific parts of an error function until the error value goes to 0. This error\n",
        "function is calculated using a combination of variables, some of which you can change (weights) and\n",
        "some of which you can’t (input data, output data, and the error logic):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDaZ2EKL8Z_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight = 0.5\n",
        "goal_pred = 0.8\n",
        "input = 0.5\n",
        "\n",
        "for i in range(20):\n",
        "  print(f'-----------\\nWeight: {str(weight)}')\n",
        "\n",
        "  # These lines have a secret.\n",
        "  pred = input * weight             # multiply weight parameter and input feature\n",
        "  error = (pred - goal_pred) ** 2   # measure the diffrerence of predicted value and target value\n",
        "\n",
        "  # get the diffrerence(missing value) of predicted value and target value\n",
        "  # multiply again input feature by the diffrerence(missing value)\n",
        "  direction_and_amount = (pred - goal_pred) * input    \n",
        "  # adjust the weight by subtracting with multiplied input feature and the diffrerence(missing value)  \n",
        "  weight = weight - direction_and_amount   \n",
        "  print(f'Error: {str(error)}  Prediction: {str(pred)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKDH5XR1EJSY",
        "colab_type": "text"
      },
      "source": [
        "Learning is all about automatically changing the prediction function so that it\n",
        "makes good predictions—aka, so that the subsequent error goes down to 0.\n",
        "\n",
        "Now that you know what you’re allowed to change, how do you go about doing the changing?\n",
        "That’s the good stuff. That’s the machine learning, right? In the next section, we’re going to talk\n",
        "about exactly that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNFJJX0SEfHE",
        "colab_type": "text"
      },
      "source": [
        "##Tunnel vision on one concept\n",
        "**Concept: Learning is adjusting the weight to reduce the error to 0.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txGPt0iEEmGK",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTb1lD5hEAua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}